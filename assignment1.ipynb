{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook created by Anirudh Swaminathan from ECE department majoring in Intelligent Systems, Robotics and Control for the course ECE285 Machine Learning for Image Processing for Fall 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import MNISTtools\n",
    "help(MNISTtools.load)\n",
    "help(MNISTtools.show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "xtrain, ltrain = MNISTtools.load(path='./datasets/MNIST')\n",
    "print(xtrain.shape)\n",
    "print(ltrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of $xtrain$ is $(784, 60000)$<br>\n",
    "The shape of $ltrain$ is $(60000, )$<br>\n",
    "The size of the training set, i.e., the number of images in the training set is $60000$<br>\n",
    "The feature dimension is $784$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Displaying the image at index 42\n",
    "MNISTtools.show(xtrain[:, 42])\n",
    "\n",
    "# Print its corresponding label\n",
    "print(ltrain[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image at the index $42$ has been displayed.<br>\n",
    "The corresponding label has been printed and is found to be $7$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the range and type of xtrain\n",
    "min_x = np.amin(xtrain)\n",
    "max_x = np.amax(xtrain)\n",
    "\n",
    "print(\"Range of xtrain is from \", min_x, \" to \", max_x)\n",
    "print(\"Data type of xtrain is \", xtrain.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of values for $xtrain$ is from $0$ to $255$<br>\n",
    "The type of $xtrain$ is $uint8$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_MNIST_images(x):\n",
    "    # Convert the uint8 input into float32 for ease of normalization\n",
    "    fl_x = x.astype(np.float32)\n",
    "    \n",
    "    # Normalize [0 to 255] to [-1 to 1]\n",
    "    # This means mapping 0 to -1, 255 to 1, and 127.5 to 0\n",
    "    # ret = 2*(fl_x - 255/2.0) / 255\n",
    "    ret = -1 + 2*fl_x / 255\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_x_train = normalize_MNIST_images(xtrain)\n",
    "print(norm_x_train.shape)\n",
    "print(\"Range of normalized xtrain is\", np.amin(norm_x_train), \"to\", np.amax(norm_x_train))\n",
    "print(\"Data type of normalized xtrain is\", norm_x_train.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrote the function to normalize the training data from $[0 to 255]$ to $[-1 to 1]$<br>\n",
    "We converted $xtrain$ which was of type $uint8$ into a vector of type $float32$<br>\n",
    "We then mapped $0$ to $-1$, $255$ to $1$ by subtracting the mid, which is $127.5$ and then dividing by mid, which is $127.5$<br>\n",
    "We then stored the normalized $xtrain$ in the variable $norm\\_x\\_train$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the code below\n",
    "def label2onehot(lbl):\n",
    "    # Creates a placeholder of size (10, 60000)\n",
    "    d = np.zeros((lbl.max() + 1, lbl.size))\n",
    "    \n",
    "    # One-hot encode the labels\n",
    "    d[lbl, np.arange(lbl.size)] = 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = label2onehot(ltrain)\n",
    "print(dtrain.shape)\n",
    "print(np.amin(dtrain), np.amax(dtrain))\n",
    "print(\"Label at index 42 is\", ltrain[42])\n",
    "print(\"Corresponding one-hot encodiing is\", dtrain[:, 42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one hot encoding line works as the $1^{st}$ index is traveresed independently of the $2^{nd}$ index<br>\n",
    "So, for each image given by the $2^{nd}$ axis, only the row index given by the value of the label is assigned $1$<br>\n",
    "Thus, $0$ maps to $[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]$ and 9 maps to $[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]$<br><br>\n",
    "We also checked the label for image $42$. The label is $7$ and the corresponding one-hot encoding is $[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot2label(d):\n",
    "    lbl = d.argmax(axis=0)\n",
    "    return lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if this works\n",
    "lab = dtrain[:, 42]\n",
    "che = onehot2label(lab)\n",
    "\n",
    "print(\"One-hot answer\", che, \"| Original:\", ltrain[42])\n",
    "assert(che == ltrain[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have thus checked if our implementation of recovering the label from one-hot encoding is correct<br>\n",
    "The label of the image at index at $42$ is $7$<br>\n",
    "The $onehot2label()$ function recovers this correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the softmax function\n",
    "def softmax(a):\n",
    "    # Calculate the max value\n",
    "    M = np.max(a, axis=0)\n",
    "    \n",
    "    # Subtract for easier exponential calculation\n",
    "    a_m = a - M\n",
    "    \n",
    "    # Calculate the exponent for each class for each image\n",
    "    exp_a_m = np.exp(a_m)\n",
    "    \n",
    "    # Calculate the sum for each class\n",
    "    den = np.sum(exp_a_m, axis=0)\n",
    "    \n",
    "    # Get the probabilities for each class for each image\n",
    "    g_a = exp_a_m / den\n",
    "    return g_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to show that $$\\frac{\\partial{g(a)_i}}{\\partial{a_i}} = g(a)_i(1 - g(a)_i)$$<br>\n",
    "By definition above, Softmax is $$y_i = g(a)_i = \\frac{exp(a_i)}{\\sum_{j=1}^{10}exp(a_j)} $$\n",
    "So, $$ \\frac{\\partial{g(a)_i}}{\\partial{a_i}} = \\frac{\\partial \\left({\\frac{exp(a_i)}{\\sum_{j=1}^{10}exp(a_j)}} \\right)}{\\partial{a_i}} $$\n",
    "Using the division rule of derivatives, we have $$ \\frac{\\partial{g(a)_i}}{\\partial{a_i}} = \\frac{\\sum_{j=1}^{10}exp(a_j)\\frac{\\partial{exp(a_i)}}{\\partial{a_i}} - exp(a_i)\\frac{\\partial \\left( {\\sum_{j=1}^{10}exp(a_j)} \\right)}{\\partial{a_i}}}{\\left( \\sum_{j=1}^{10}exp(a_j) \\right)^2} $$\n",
    "Simplifying, we have $$ \\frac{\\partial{g(a)_i}}{\\partial{a_i}} = \\frac{exp(a_i)* \\sum_{j=1}^{10}exp(a_j) - exp(a_i)*exp(a_i)}{\\left( \\sum_{j=1}^{10}exp(a_j) \\right)^2} $$\n",
    "Taking $\\frac{exp(a_i)}{\\sum_{j=1}^{10}exp(a_j)}$ outside, we have $$ \\frac{\\partial{g(a)_i}}{\\partial{a_i}} = \\frac{exp(a_i)}{\\sum_{j=1}^{10}exp(a_j)} * \\left( \\frac{\\sum_{j=1}^{10}exp(a_j) - exp(a_i)}{\\left( \\sum_{j=1}^{10}exp(a_j) \\right)} \\right) $$\n",
    "We know that $ g(a)_i = \\frac{exp(a_i)}{\\sum_{j=1}^{10}exp(a_j)} $<br>\n",
    "Thus, we have $$ \\frac{\\partial{g(a)_i}}{\\partial{a_i}} = g(a)_i * \\left( 1 - g(a)_i \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to show that $$\\frac{\\partial{g(a)_i}}{\\partial{a_j}} = -g(a)_i*g(a)_j for j\\neq i$$<br>\n",
    "By definition above, Softmax is $$y_i = g(a)_i = \\frac{exp(a_i)}{\\sum_{j=1}^{10}exp(a_j)} $$\n",
    "So, $$ \\frac{\\partial{g(a)_i}}{\\partial{a_j}} = \\frac{\\partial \\left({\\frac{exp(a_i)}{\\sum_{j=1}^{10}exp(a_j)}} \\right)}{\\partial{a_j}} $$\n",
    "Taking the term $exp(a_i)$ outside, we have, $$ \\frac{\\partial{g(a)_i}}{\\partial{a_j}} = exp(a_i) * \\frac{\\partial \\left({\\frac{1}{\\sum_{j=1}^{10}exp(a_j)}} \\right)}{\\partial{a_j}} $$\n",
    "Using inverse rule of derivatives, we have $$ \\frac{\\partial{g(a)_i}}{\\partial{a_j}} = exp(a_i) * \\frac{-1*exp(a_j)}{\\left( \\sum_{j=1}^{10}exp(a_j) \\right)^2} $$\n",
    "We know that $ g(a)_i = \\frac{exp(a_i)}{\\sum_{j=1}^{10}exp(a_j)} $<br>\n",
    "Thus, we have $$ \\frac{\\partial{g(a)_i}}{\\partial{a_j}} = -1*g(a)_i*g(a)_j for j\\neq i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - Jacobian is symmetric Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - Jacobian expression proof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the gradient of the softmax function\n",
    "# The directional derivative of the softmax function is as follows:-\n",
    "# delta = elementwise product (g(a) and e) - <g(a),e> g(a)\n",
    "def softmaxp(a, e):\n",
    "    # Calculate g(a)\n",
    "    g_a = softmax(a)\n",
    "    \n",
    "    # Calculate term 1\n",
    "    t1 = g_a * e\n",
    "    \n",
    "    # Calculate the directional derivative\n",
    "    delta = t1 - np.sum(t1, axis=0)*g_a\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if softmaxp is correct\n",
    "# finite difference step\n",
    "eps = 1e-6\n",
    "\n",
    "# random inputs\n",
    "a = np.random.randn(10, 200)\n",
    "\n",
    "# random directions\n",
    "e = np.random.randn(10, 200)\n",
    "\n",
    "# testing part\n",
    "diff = softmaxp(a, e)\n",
    "\n",
    "# From the definition of a derivative, we have\n",
    "diff_approx = (softmax(a + eps*e) - softmax(a)) / eps\n",
    "\n",
    "# Calculate the relative error of these 2 approaches\n",
    "rel_error = np.abs(diff - diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "\n",
    "# print the relative error\n",
    "print(rel_error, 'should be smaller than 1e-6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented the code to compute the directional derivative of $g$ at point $a$ in the direction of $e$ using the $softmaxp(a, e)$ function<br>\n",
    "We tested the implementation of our code by comparing with the fundamental definition of directional derivative, where, $$ \\delta = \\frac{\\partial g(a)}{\\partial a} \\times e = \\lim_{\\varepsilon\\to0} \\frac{g(a + \\varepsilon e) - g(a)}{\\varepsilon} $$\n",
    "We verified that our implementation of $softmaxp()$ is correct and that the relative error is smaller that $1e-6$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the ReLU(a) = max(ai, 0)\n",
    "def relu(a):\n",
    "    # Create a copy of the array a\n",
    "    #g_a = np.copy(a)\n",
    "    \n",
    "    # Set those values less than 0 to 0\n",
    "    #g_a[a < 0] = 0\n",
    "    #return g_a\n",
    "    return np.maximum(a, 0)\n",
    "\n",
    "def relup(a, e):\n",
    "    # Relup is the directional derivative of ReLU(a) in the direction of e\n",
    "    # Taking the Jacobian for ReLU and then deriving, we have found that the derivative is as given:-\n",
    "    # It is the element-wise product of gradient of relu and the vector e\n",
    "    # Create a copy of the array a\n",
    "    del_a = np.copy(a)\n",
    "    \n",
    "    # Set the values less than 0 to 0\n",
    "    del_a[a < 0] = 0\n",
    "    \n",
    "    # Set the values greater than 0 to 1\n",
    "    del_a[a > 0] = 1\n",
    "    \n",
    "    # Compute delta as the element-wise product of the gradient of relu and the vector e\n",
    "    delta = del_a * e\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented the relu function and its directional derivative now<br>\n",
    "We used the Jacobian to derive the relation of relup to vector operations<br>\n",
    "We shall now test $reulp()$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if relup is correct\n",
    "# finite difference step\n",
    "eps = 1e-6\n",
    "\n",
    "# random inputs\n",
    "a = np.random.randn(10, 200)\n",
    "\n",
    "# random directions\n",
    "e = np.random.randn(10, 200)\n",
    "\n",
    "# testing part\n",
    "diff = relup(a, e)\n",
    "\n",
    "# From the definition of a derivative, we have\n",
    "diff_approx = (relu(a + eps*e) - relu(a)) / eps\n",
    "\n",
    "# Calculate the relative error of these 2 approaches\n",
    "rel_error = np.abs(diff - diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "\n",
    "# print the relative error\n",
    "print(rel_error, 'should be smaller than 1e-6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented the code to compute the directional derivative of $g$ at point $a$ in the direction of $e$ using the $relup(a, e)$ function<br>\n",
    "We tested the implementation of our code by comparing with the fundamental definition of directional derivative, where, $$ \\delta = \\frac{\\partial g(a)}{\\partial a} \\times e = \\lim_{\\varepsilon\\to0} \\frac{g(a + \\varepsilon e) - g(a)}{\\varepsilon} $$\n",
    "We verified that our implementation of $relup()$ is correct and that the relative error is smaller that $1e-6$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and initialize our shallow network\n",
    "def init_shallow(Ni, Nh, No):\n",
    "    \"\"\"\n",
    "    Ni - dimension of the input layer. Ni = 784\n",
    "    Nh - dimension of the hidden layer. Nh = 64\n",
    "    No - dimension of the output layer. No = 10\n",
    "    \"\"\"\n",
    "    # Create the bias vector for the 1st layer\n",
    "    # We are using He initialization method\n",
    "    b1 = np.random.randn(Nh, 1) / np.sqrt((Ni + 1.) / 2.)\n",
    "    # Create the synaptic weights between the input and the hidden neurons\n",
    "    W1 = np.random.randn(Nh, Ni) / np.sqrt((Ni + 1.) / 2.)\n",
    "    \n",
    "    # Create the bias vector for the 2nd layer\n",
    "    # We are using Xavier initialization method\n",
    "    b2 = np.random.randn(No, 1) / np.sqrt((Nh + 1.))\n",
    "    # Create the synaptic weights between the hidden and the output neurons\n",
    "    W2 = np.random.randn(No, Nh) / np.sqrt((Nh + 1.))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Initialize our shallow network\n",
    "Ni = norm_x_train.shape[0]\n",
    "Nh = 64\n",
    "No = dtrain.shape[0]\n",
    "netinit = init_shallow(Ni, Nh, No)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined the network architecture and parameters and initialized them in the snippet above<br>\n",
    "We used He initialization for the input neurons to hidden neurons connections, and we used Xavier initialization for the hidden neurons to output neurons connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the forward_prop function to propagate the activations through the network\n",
    "def forwardprop_shallow(x, net):\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    \n",
    "    # Input to hidden neurons\n",
    "    a1 = W1.dot(x) + b1\n",
    "    h1 = relu(a1)\n",
    "    \n",
    "    # Hidden to output neurons\n",
    "    a2 = W2.dot(h1) + b2\n",
    "    y = softmax(a2)\n",
    "    return y\n",
    "\n",
    "# Calculate the initial output for the random initializations\n",
    "yinit = forwardprop_shallow(norm_x_train, netinit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the forward_prop function to propagate the activations through the network\n",
    "# This is very useful for backprop, as all the network activations are returned\n",
    "def fp_shallow(x, net):\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    \n",
    "    # Input to hidden neurons\n",
    "    a1 = W1.dot(x) + b1\n",
    "    h1 = relu(a1)\n",
    "    \n",
    "    # Hidden to output neurons\n",
    "    a2 = W2.dot(h1) + b2\n",
    "    y = softmax(a2)\n",
    "    return h1, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(norm_x_train.shape)\n",
    "print(yinit.shape)\n",
    "\n",
    "print(np.min(norm_x_train), np.max(norm_x_train))\n",
    "print(np.min(yinit), np.max(yinit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented the function to propagate forward through the network<br>\n",
    "We subsequently calculated the initial output for our initialization of the network with random parameter values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the cross-entropy loss\n",
    "def eval_loss(y, d):\n",
    "    # Calculates the log of the predicted probabilities\n",
    "    log_y = np.log(y)\n",
    "    \n",
    "    # Element-wise multiplication with d\n",
    "    mult = d*log_y\n",
    "    \n",
    "    # Take the negative to get cross-entropy\n",
    "    mult = -1 * mult\n",
    "    \n",
    "    # calculate the sum over all probabilities and sum over all the input vectors\n",
    "    sum_pro = np.sum(mult)\n",
    "    \n",
    "    # Calculate the average of the cross-entropy\n",
    "    ret = np.mean(mult)\n",
    "    return ret\n",
    "\n",
    "# Check the evaluation loss for the initial predictions\n",
    "print(eval_loss(yinit, dtrain), 'should be around .26')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have thus implemented the function to calculate the loss<br>\n",
    "We have verified that the initial loss is around .26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate the percentage\n",
    "def eval_perfs(y, lbl):\n",
    "    # Convert the given probabilities to corresponding label\n",
    "    pred_lbl = onehot2label(y)\n",
    "    \n",
    "    # Compare the groundtruth with the predicted label and identify Misclassified samples\n",
    "    comps = [pred_lbl != lbl]\n",
    "    nums = np.sum(comps)\n",
    "    ret = (nums * 1.0 / lbl.shape[0]) * 100.0\n",
    "    return ret\n",
    "\n",
    "# Print the percentage of \"mis-classified\" samples for the initial predicted probabilities\n",
    "# and the groundtruth labels\n",
    "print(eval_perfs(yinit, ltrain), \"% of the images are misclassified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented the function to calculate the percentage of mis-classified samples<br>\n",
    "We picked the index with the maximum value(probability) for each column using the $y.argmax(axis=0)$ function<br>\n",
    "This index is basically the predicted class of the given image(column)<br>\n",
    "We then compared the predicted label with the actual label, calculated the number of misclassified images, and then divided by the total number of images to get the percentage of mis-classification<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to show that $$ \\left( \\nabla_yE \\right)_i =  -\\frac{d_i}{y_i} $$\n",
    "$E$ is cross-entropy loss, and is given by $$ E = - \\sum_{i=1}^{10} d_{i}log(y_{i}) $$\n",
    "Differentiating with respect to $y_i$ and taking $-d_i$ as common, we have $$ \\left( \\nabla_yE \\right)_i = -d_i * \\frac{\\partial \\left( \\sum_{i=1}^{10}log(y_i) \\right)}{\\partial y_i} $$\n",
    "Thus, we have $$ \\left( \\nabla_yE \\right)_i = -d_i * \\left( \\frac{1}{y_i} \\right) $$\n",
    "Hence, proved that $$ \\left( \\nabla_yE \\right)_i =  -\\frac{d_i}{y_i} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform backpropagation in the network\n",
    "def update_shallow(x, d, net, gamma=.05):\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    \n",
    "    Ni = W1.shape[1]\n",
    "    Nh = W1.shape[0]\n",
    "    No = W2.shape[0]\n",
    "    \n",
    "    # Normalize the gamma by the training dataset size\n",
    "    gamma = gamma / x.shape[1]\n",
    "    \n",
    "    ## Backprop begins!\n",
    "    # forward prop through the network using current parameters\n",
    "    # This calculates the predicted probabilities for each class\n",
    "    # working dim - 64*60000 - h1; 10*60000 - y_pred\n",
    "    # fp_shallow() is our custom forward prop that returns both the outputs of the\n",
    "    # output neurons as well as the hidden neurons\n",
    "    #h1, y_pred = fp_shallow(x, net)\n",
    "    a1 = W1.dot(x) + b1\n",
    "    h1 = relu(a1)\n",
    "    \n",
    "    # Hidden to output neurons\n",
    "    a2 = W2.dot(h1) + b2\n",
    "    y = softmax(a2)\n",
    "    \n",
    "    # Calculate the loss\n",
    "    # e = eval_loss(y_pred, dtrain)\n",
    "    \n",
    "    ## Backprop through output neurons to hidden neurons\n",
    "    # Calculate the gradient of the error for output neurons\n",
    "    # working dim - 10*60000\n",
    "    #print(d.shape, y_pred.shape)\n",
    "    #print(np.min(d), np.max(d))\n",
    "    #print(np.min(y_pred), np.max(y_pred))\n",
    "    #e2 = -1.0 * d / y_pred\n",
    "    #e2 = -d/y_pred\n",
    "    e2 = -d/y\n",
    "    #e2 = -1.0 * np.divide(d, y_pred, out=np.zeros_like(d), where=d!=0)\n",
    "    \n",
    "    # calculate derivative of softmax() activation\n",
    "    # working dim - 10*60000\n",
    "    #delta2 = softmaxp(y_pred, e2)\n",
    "    delta2 = softmaxp(y, e2)\n",
    "    \n",
    "    # Calculate the derivative of E wrt W2\n",
    "    # working dim - 10*60000 * 60000*64(h1.T)\n",
    "    grad_w2_e = delta2.dot(h1.T)\n",
    "    \n",
    "    # Calculate the derivative of E wrt b2\n",
    "    # working dim - 10*60000 * 60000*1 = 10*1\n",
    "    grad_b2_e = delta2.dot(np.ones((delta2.shape[1], 1)))\n",
    "    \n",
    "    # Calculate the gradient of the error for the hidden neurons\n",
    "    # Working dim - 64*60000\n",
    "    # 64*10(W2 is 10*64) * 10*60000\n",
    "    e1 = W2.T.dot(delta2)\n",
    "    \n",
    "    # Calculate the derivative of the relu() activation\n",
    "    # working dim - 64*60000\n",
    "    delta1 = relup(h1, e1)\n",
    "    \n",
    "    # Calculate the derivative of E wrt W1\n",
    "    # working dim - 64*60000 * 60000*784(x.T) (h0 = x)\n",
    "    grad_w1_e = delta1.dot(x.T)\n",
    "    \n",
    "    # Calculate the derivative of E wrt b1\n",
    "    # working dim - 64*60000 * 60000*1 = 64*1\n",
    "    grad_b1_e = delta1.dot(np.ones((delta1.shape[1], 1)))\n",
    "    \n",
    "    ## UPDATE the parameters\n",
    "    W2 = W2 - gamma * grad_w2_e\n",
    "    W1 = W1 - gamma * grad_w1_e\n",
    "    b2 = b2 - gamma * grad_b2_e\n",
    "    b1 = b1 - gamma * grad_b1_e\n",
    "    \n",
    "    # return the updated parameters\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we have written the function to perform one backpropagation update for our shallow network.\n",
    "We have also proved that $$ \\left( \\nabla_yE \\right)_i =  -\\frac{d_i}{y_i} $$\n",
    "We have also written a $fp_shallow()$ function that returns the activations of both the hidden neurons and the otuput neurons that is used for our backpropagation<br>\n",
    "We then used $softmaxp()$ and $relup()$ to calculate the gradients<br>\n",
    "We implemented the backpropagation layer-wise from the output neurons to the hidden neurons.<br>\n",
    "We coded the backpropagation as given:-\n",
    "$$ W_k^{t+1} = W_k^t - \\gamma \\nabla_{w_k}E^t $$\n",
    "$$ b_k^{t+1} = b_k^t - \\gamma \\nabla_{b_k}E^t $$\n",
    "$$ where \\quad \\nabla_{w_k}E = \\delta_k h_{k-1}^T $$\n",
    "$$ and \\quad \\nabla_{b_k}E = \\delta_k 1_N $$\n",
    "$$ and \\quad \\delta_k = \\left[ \\frac{\\partial g_k(a_k)}{\\partial a_k} \\right]^T \\times e_k $$\n",
    "$$ where \\quad e_k = \\left\\{ \\begin{array}{ll} \\nabla_y E \\quad \\text{if k is an output layer} \\\\ W_{k+1}^T \\delta_{k+1} \\quad \\text{otherwise} \\end{array} \\right.$$\n",
    "We finally return the network parameters $W_1, b_1, W_2 and b_2$ after one iteration of backpropagation to the caller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute backprop_shallow\n",
    "def backprop_shallow(x, d, net, T, gamma=.05):\n",
    "    # Get the label given the one-hot encoding\n",
    "    lbl = onehot2label(d)\n",
    "    \n",
    "    # Compute and display the loss and performance measure initially\n",
    "    y = forwardprop_shallow(x, net)\n",
    "    tr_loss = eval_loss(y, d)\n",
    "    print(\"Initial loss is:\", tr_loss)\n",
    "    tr_perf = eval_perfs(y, lbl)\n",
    "    print(tr_perf, \"% of images are misclassified initially\\n\")\n",
    "    \n",
    "    for t in range(T):\n",
    "        # update the parameters using the update_shallow() function\n",
    "        net = update_shallow(x, d, net, gamma)\n",
    "        \n",
    "        # Compute and display the loss and performance measure for each iteration\n",
    "        y = forwardprop_shallow(x, net)\n",
    "        tr_loss = eval_loss(y, d)\n",
    "        print(\"Training loss after iteration\", t+1, \"is:\", tr_loss)\n",
    "        tr_perf = eval_perfs(y, lbl)\n",
    "        print(tr_perf, \"% of images are misclassified after iteration\", t+1,\"\\n\")\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the net for 2 iterations initially. The output is the final parameters after training\n",
    "nettrain = backprop_shallow(norm_x_train, dtrain, netinit, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrote the code for $backprop\\_shallow()$ to train the network<br>\n",
    "This function performs $T$ updates of the network by calling one instance of the backpropagation function $update\\_shallow()$ each time<br>\n",
    "We evaluate the loss initially and after each iteration by calling the $eval_loss()$ function and then display it<br>\n",
    "Similarly, we evaluate the percentage of images mis-classified initially and after each iteration by calling the $eval_perfs()$ function and then display it<br>\n",
    "This function finally returns the completely trained parameters $W_1, b_1, W_2, b_2$ after $T$ iterations and stores it in $nettrain$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried running  the code with $2$ iterations initially.<br>\n",
    "$2$ iterations worked in reducing the loss from $0.279$ to $0.221$<br>\n",
    "It also reduced the percentage of misclassified images from $89.51\\%$ to $85.50\\%$<br>\n",
    "So we moved onto testing the function with $5$ iterations<br><br>\n",
    "$5$ iterations worked in reducing the loss from $0.276$ to $0.208$<br>\n",
    "It also reduced the percentage of misclassified images from $88.99\\%$ to $69.66\\%$<br>\n",
    "So, we moved onto testing the function with $20$ iterations<br><br>\n",
    "$20$ iterations worked in reducing the loss from $0.254$ to $0.126$<br>\n",
    "It also reduced the percentage of misclassified images from $89.005\\%$ to $32.71\\%$<br>\n",
    "We finally run the network with $100$ iterations<br><br>\n",
    "$100$ iterations worked in reducing the loss from $some$ to $thing$<br>\n",
    "It also reduced the percentage of misclassified images from $per\\%$ to $cent\\%$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the testing data\n",
    "xtest, ltest = MNISTtools.load(dataset='testing', path='./datasets/MNIST')\n",
    "print(xtest.shape)\n",
    "print(ltest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the testing set of images is found to be $(784, 10000)$, that is, it contains $10000$ images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the image at index 42\n",
    "MNISTtools.show(xtest[:, 42])\n",
    "\n",
    "# Print its corresponding label\n",
    "print(ltest[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the range and type of xtest\n",
    "min_te_x = np.amin(xtest)\n",
    "max_te_x = np.amax(xtest)\n",
    "\n",
    "print(\"Range of xtest is from \", min_te_x, \" to \", max_te_x)\n",
    "print(\"Data type of xtest is \", xtest.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the test images\n",
    "norm_x_test = normalize_MNIST_images(xtest)\n",
    "print(norm_x_test.shape)\n",
    "print(\"Range of normalized xtest is\", np.amin(norm_x_test), \"to\", np.amax(norm_x_test))\n",
    "print(\"Data type of normalized xtest is\", norm_x_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest = label2onehot(ltest)\n",
    "print(dtest.shape)\n",
    "print(np.amin(dtest), np.amax(dtest))\n",
    "print(\"Label at index 42 is\", ltest[42])\n",
    "print(\"Corresponding one-hot encodiing is\", dtest[:, 42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display the loss and performance measure on the test set\n",
    "y_test = forwardprop_shallow(norm_x_test, nettrain)\n",
    "te_loss = eval_loss(y_test, dtest)\n",
    "print(\"Test loss is:\", te_loss)\n",
    "te_lbl = onehot2label(dtest)\n",
    "te_perf = eval_perfs(y_test, te_lbl)\n",
    "print(te_perf, \"% of images are misclassified in the test set\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we have loaded the testing dataset<br>\n",
    "We tested the performance of the network parameters that were trained for $2$ iterations and we got a loss of $0.221$ on this test set<br>\n",
    "Training loss after $2$ iterations was: $0.221$<br>\n",
    "$84.99\\%$ of the images are mis-classified from the test set after the training<br>\n",
    "$85.50\\%$ of images are misclassified in the training set after $2$ iterations<br><br>\n",
    "We tested the performance of the network parameters that were trained for $5$ iterations and we got a loss of $5_iter$ on this test set<br>\n",
    "Training loss after $5$ iterations was: $5_iter$<br>\n",
    "$5_iter\\%$ of the images are mis-classified from the test set after the training<br>\n",
    "$5_iter\\%$ of images are misclassified in the training set after $5$ iterations<br><br>\n",
    "We tested the performance of the network parameters that were trained for $20$ iterations and we got a loss of $0.123$ on this test set<br>\n",
    "Training loss after $20$ iterations was: $0.126$<br>\n",
    "$31.79\\%$ of the images are mis-classified from the test set after the training<br>\n",
    "$33.03\\%$ of images are misclassified in the training set after $20$ iterations<br><br>\n",
    "We tested the performance of the network parameters that were trained for $100$ iterations and we got a loss of $100_iter$ on this test set<br>\n",
    "Training loss after $100$ iterations was: $100_iter$<br>\n",
    "$31.79\\%$ of the images are mis-classified from the test set after the training<br>\n",
    "$33.03\\%$ of images are misclassified in the training set after $100$ iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backprop using minibatch\n",
    "def backprop_minibatch_shallow(x, d, net, T, B=100, gamma=0.05):\n",
    "    # Get the number of images\n",
    "    N = x.shape[1]\n",
    "    \n",
    "    # Calculate the number of batches\n",
    "    NB = int((N+B-1)/B)\n",
    "    \n",
    "    # Convert one-hot encoded data to a label\n",
    "    lbl = onehot2label(d)\n",
    "    \n",
    "    # Compute and display the loss and performance measure initially\n",
    "    # y = forwardprop_shallow(x, net)\n",
    "    # tr_mini_loss = eval_loss(y, d)\n",
    "    # print(\"Initial minibatch loss is:\", tr_mini_loss)\n",
    "    # tr_mini_perf = eval_perfs(y, lbl)\n",
    "    # print(tr_mini_perf, \"% of images are misclassified initially using minibatch method\\n\")\n",
    "    \n",
    "    # For every iteration(epoch)\n",
    "    for t in range(T):\n",
    "        # shuffle the indices to access the data\n",
    "        shuffled_indices = np.random.permutation(range(N))\n",
    "        \n",
    "        # For each minibatch\n",
    "        for l in range(NB):\n",
    "            # get the shuffled indices for a given minibatch\n",
    "            minibatch_indices = shuffled_indices[B*l:min(B*(l+1), N)]\n",
    "            \n",
    "            # Backprop through the minibatch and update the parameters of the network\n",
    "            net = update_shallow(x[:, minibatch_indices], d[:, minibatch_indices], net, gamma)\n",
    "            \n",
    "        y = forwardprop_shallow(x, net)\n",
    "        tr_mini_loss = eval_loss(y, d)\n",
    "        print(\"Training loss using minibatches after epoch\", t+1, \"is:\", tr_mini_loss)\n",
    "        tr_mini_perf = eval_perfs(y, lbl)\n",
    "        print(tr_mini_perf, \"% of images are misclassified using minibatches after epoch\", t+1,\"\\n\")\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrote the code for $backprop\\_minibatch\\_shallow()$ to train the network<br>\n",
    "In minibatch backpropagation, we divided the dataset into a number of mini-batches, each sized $100$ images<br>\n",
    "We thus updated the parameters of our network TN/B times<br>\n",
    "We first calculate the number of batches to train for<br>\n",
    "We then shuffle the entire dataset, and for each minibatch, we update the parameters of the network using $update_shallow()$ function<br>\n",
    "We evaluate the loss initially and after each epoch by calling the $eval_loss()$ function and then display it<br>\n",
    "Similarly, we evaluate the percentage of images mis-classified initially and after each epoch by calling the $eval_perfs()$ function and then display it<br>\n",
    "This function finally returns the completely trained parameters $W_1, b_1, W_2, b_2$ after $T$ epochs and stores it in $netminibatch$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network for a few epochs\n",
    "print(np.min(norm_x_train), np.max(norm_x_train))\n",
    "netminibatch = backprop_minibatch_shallow(norm_x_train, dtrain, netinit, 2, B=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried running  the code with $5$ epochs initially.<br>\n",
    "$5$ epochs using minibatches worked in reducing the loss from $0.271$ to $0.238$<br>\n",
    "Using minibatches also reduced the percentage of misclassified images from $88.99\\%$ to $69.66\\%$\n",
    "\n",
    "This is different from running with $5$ iterations over the whole training set<br>\n",
    "$5$ iterations worked in reducing the loss from $0.276$ to $0.208$<br>\n",
    "It also reduced the percentage of misclassified images from $88.99\\%$ to $69.66\\%$<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display the loss and performance measure on the test set\n",
    "y_mini_test = forwardprop_shallow(norm_x_test, netminibatch)\n",
    "te_mini_loss = eval_loss(y_mini_test, dtest)\n",
    "print(\"Test loss after minibatch gradient descent is:\", te_mini_loss)\n",
    "te_mini_perf = eval_perfs(y_mini_test, te_lbl)\n",
    "print(te_mini_perf, \"% of images are misclassified in the test set after minibatch gradient descent\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tested the performance of the network parameters that were trained for $5$ epochs and we got a loss of $0.123$ on this test set<br>\n",
    "Training loss after $5$ epochs was: $0.126$<br>\n",
    "$31.79\\%$ of the images are mis-classified from the test set after the minibatch training<br>\n",
    "$33.03\\%$ of images are misclassified in the training set after $5$ epochs<br>\n",
    "\n",
    "Testing loss after training on the entire network for $5$ iterations was : $0.126$<br>\n",
    "$5_iter\\%$ of the images are mis-classified from the test set after the entire training<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the performance of the network using minbatches vs. the entire network, we conclude that ___ gives slightly better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we have learnt about shallow networks, and implemented a simple shallow feedforward network to classify MNIST handwritten images. We trained the whole network over multiple iterations, as well as trained it using minibatches and compared their performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment completed by\n",
    " - Name: Anirudh Swaminathan\n",
    " - PID: A53316083\n",
    " - Email ID: aswamina@eng.ucsd.edu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
