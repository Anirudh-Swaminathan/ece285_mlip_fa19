{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook created by Anirudh Swaminathan from ECE department majoring in Intelligent Systems, Robotics and Control for the course ECE285 Machine Learning for Image Processing for Fall 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MNISTtools\n",
    "help(MNISTtools.load)\n",
    "help(MNISTtools.show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "xtrain, ltrain = MNISTtools.load(path='./datasets/MNIST')\n",
    "print(xtrain.shape)\n",
    "print(ltrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of $xtrain$ is $(784, 60000)$<br>\n",
    "The shape of $ltrain$ is $(60000, )$<br>\n",
    "The size of the training set, i.e., the number of images in the training set is $60000$<br>\n",
    "The feature dimension is $784$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the image at index 42\n",
    "MNISTtools.show(xtrain[:, 42])\n",
    "\n",
    "# Print its corresponding label\n",
    "print(ltrain[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image at the index $42$ has been displayed.<br>\n",
    "The corresponding label has been printed and is found to be $7$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the range and type of xtrain\n",
    "min_x = np.amin(xtrain)\n",
    "max_x = np.amax(xtrain)\n",
    "\n",
    "print(\"Range of xtrain is from \", min_x, \" to \", max_x)\n",
    "print(\"Data type of xtrain is \", xtrain.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of values for $xtrain$ is from $0$ to $255$<br>\n",
    "The type of $xtrain$ is $uint8$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_MNIST_images(x):\n",
    "    # Convert the uint8 input into float32 for ease of normalization\n",
    "    fl_x = x.astype(np.float32)\n",
    "    \n",
    "    # Normalize [0 to 255] to [-1 to 1]\n",
    "    # This means mapping 0 to -1, 255 to 1, and 127.5 to 0\n",
    "    ret = 2*(fl_x - 255/2.0) / 255\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_x_train = normalize_MNIST_images(xtrain)\n",
    "print(norm_x_train.shape)\n",
    "print(\"Range of normalized xtrain is\", np.amin(norm_x_train), \"to\", np.amax(norm_x_train))\n",
    "print(\"Data type of normalized xtrain is\", norm_x_train.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrote the function to normalize the training data from $[0 to 255]$ to $[-1 to 1]$<br>\n",
    "We converted $xtrain$ which was of type $uint8$ into a vector of type $float32$<br>\n",
    "We then mapped $0$ to $-1$, $255$ to $1$ by subtracting the mid, which is $127.5$ and then dividing by mid, which is $127.5$<br>\n",
    "We then stored the normalized $xtrain$ in the variable $norm\\_x\\_train$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the code below\n",
    "def label2onehot(lbl):\n",
    "    # Creates a placeholder of size (10, 60000)\n",
    "    d = np.zeros((lbl.max() + 1, lbl.size))\n",
    "    \n",
    "    # One-hot encode the labels\n",
    "    d[lbl, np.arange(lbl.size)] = 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = label2onehot(ltrain)\n",
    "print(dtrain.shape)\n",
    "print(np.amin(dtrain), np.amax(dtrain))\n",
    "print(\"Label at index 42 is\", ltrain[42])\n",
    "print(\"Corresponding one-hot encodiing is\", dtrain[:, 42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one hot encoding line works as the $1^{st}$ index is traveresed independently of the $2^{nd}$ index<br>\n",
    "So, for each image given by the $2^{nd}$ axis, only the row index given by the value of the label is assigned $1$<br>\n",
    "Thus, $0$ maps to $[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]$ and 9 maps to $[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]$<br><br>\n",
    "We also checked the label for image $42$. The label is $7$ and the corresponding one-hot encoding is $[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot2label(d):\n",
    "    lbl = d.argmax(axis=0)\n",
    "    return lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if this works\n",
    "lab = dtrain[:, 42]\n",
    "che = onehot2label(lab)\n",
    "\n",
    "print(\"One-hot answer\", che, \"| Original:\", ltrain[42])\n",
    "assert(che == ltrain[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have thus checked if our implementation of recovering the label from one-hot encoding is correct<br>\n",
    "The label of the image at index at $42$ is $7$<br>\n",
    "The $onehot2label()$ function recovers this correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the softmax function\n",
    "def softmax(a):\n",
    "    # Calculate the max value\n",
    "    M = np.max(a, axis=0)\n",
    "    \n",
    "    # Subtract for easier exponential calculation\n",
    "    a_m = a - M\n",
    "    \n",
    "    # Calculate the exponent for each class for each image\n",
    "    exp_a_m = np.exp(a_m)\n",
    "    \n",
    "    # Calculate the sum for each class\n",
    "    den = np.sum(exp_a_m, axis=0)\n",
    "    \n",
    "    # Get the probabilities for each class for each image\n",
    "    g_a = exp_a_m / den\n",
    "    return g_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to show that $\\frac{\\partial{g(a)_i}}{\\partial{a_i}} = g(a)_i(1 - g(a)_i)$<br><br>\n",
    "By definition above, Softmax is $$y_i = g(a)_i = \\frac{exp(a_i)}{\\sum_{j=1}^{10}exp(a_j)} $$\n",
    "So, $$ \\frac{\\partial{g(a)_i}}{\\partial{a_i}} = \\frac{\\partial \\left({\\frac{exp(a_i)}{\\sum_{j=1}^{10}exp(a_j)}} \\right)}{\\partial{a_i}} $$\n",
    "Using the division rule of derivatives, we have $$ \\frac{\\partial{g(a)_i}}{\\partial{a_i}} = \\frac{\\sum_{j=1}^{10}exp(a_j)\\frac{\\partial{exp(a_i)}}{\\partial{a_i}} - exp(a_i)\\frac{\\partial \\left( {\\sum_{j=1}^{10}exp(a_j)} \\right)}{\\partial{a_i}}}{\\left( \\sum_{j=1}^{10}exp(a_j) \\right)^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
