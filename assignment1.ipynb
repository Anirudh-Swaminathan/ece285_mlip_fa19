{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name: Anirudh Swaminathan\n",
    "### PID: A53316083\n",
    "### Email ID: aswamina@ucsd.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook created by Anirudh Swaminathan from ECE department majoring in Intelligent Systems, Robotics and Control for the course ECE285 Machine Learning for Image Processing for Fall 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load in module MNISTtools:\n",
      "\n",
      "load(dataset='training', path=None)\n",
      "    Import either the training or testing MNIST data set.\n",
      "    It returns a pair with the first element being the collection of\n",
      "    images stacked in columns and the second element being a vector\n",
      "    of corresponding labels from 0 to 9.\n",
      "    \n",
      "    Arguments:\n",
      "        dataset (string, optional): either \"training\" or \"testing\".\n",
      "            (default: \"training\")\n",
      "        path (string, optional): the path pointing to the MNIST dataset\n",
      "            If path=None, it looks succesively for the dataset at:\n",
      "            '/datasets/MNIST' and './MNIST'. (default: None)\n",
      "    \n",
      "    Example:\n",
      "        x, lbl = load(dataset=\"testing\", path=\"/Folder/for/MNIST\")\n",
      "\n",
      "Help on function show in module MNISTtools:\n",
      "\n",
      "show(image)\n",
      "    Render a given MNIST image provided as a column vector.\n",
      "    \n",
      "    Arguments:\n",
      "        image (array): an array of shape (28*28) or (28, 28) representing a\n",
      "            grey level image of size 28 x 28. Values are expected to be in the\n",
      "            range [0, 1].\n",
      "    \n",
      "    Example:\n",
      "        x, lbl = load(dataset=\"training\", path=\"/datasets/MNIST\")\n",
      "        show(x[:, 0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import MNISTtools\n",
    "help(MNISTtools.load)\n",
    "help(MNISTtools.show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "xtrain, ltrain = MNISTtools.load(path='./datasets/MNIST')\n",
    "print(xtrain.shape)\n",
    "print(ltrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of $xtrain$ is $(784, 60000)$<br>\n",
    "The shape of $ltrain$ is $(60000, )$<br>\n",
    "The size of the training set, i.e., the number of images in the training set is $60000$<br>\n",
    "The feature dimension is $784$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# Displaying the image at index 42\n",
    "MNISTtools.show(xtrain[:, 42])\n",
    "\n",
    "# Print its corresponding label\n",
    "print(ltrain[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image at the index $42$ has been displayed.<br>\n",
    "The corresponding label has been printed and is found to be $7$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of xtrain is from  0  to  255\n",
      "Data type of xtrain is  uint8\n"
     ]
    }
   ],
   "source": [
    "# Find the range and type of xtrain\n",
    "min_x = np.amin(xtrain)\n",
    "max_x = np.amax(xtrain)\n",
    "\n",
    "print(\"Range of xtrain is from \", min_x, \" to \", max_x)\n",
    "print(\"Data type of xtrain is \", xtrain.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of values for $xtrain$ is from $0$ to $255$<br>\n",
    "The type of $xtrain$ is $uint8$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_MNIST_images(x):\n",
    "    # Convert the uint8 input into float32 for ease of normalization\n",
    "    fl_x = x.astype(np.float32)\n",
    "    \n",
    "    # Normalize [0 to 255] to [-1 to 1]\n",
    "    # This means mapping 0 to -1, 255 to 1, and 127.5 to 0\n",
    "    ret = 2*(fl_x - 255/2.0) / 255\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n",
      "Range of normalized xtrain is -1.0 to 1.0\n",
      "Data type of normalized xtrain is float32\n"
     ]
    }
   ],
   "source": [
    "norm_x_train = normalize_MNIST_images(xtrain)\n",
    "print(norm_x_train.shape)\n",
    "print(\"Range of normalized xtrain is\", np.amin(norm_x_train), \"to\", np.amax(norm_x_train))\n",
    "print(\"Data type of normalized xtrain is\", norm_x_train.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrote the function to normalize the training data from $[0 to 255]$ to $[-1 to 1]$<br>\n",
    "We converted $xtrain$ which was of type $uint8$ into a vector of type $float32$<br>\n",
    "We then mapped $0$ to $-1$, $255$ to $1$ by subtracting the mid, which is $127.5$ and then dividing by mid, which is $127.5$<br>\n",
    "We then stored the normalized $xtrain$ in the variable $norm\\_x\\_train$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the code below\n",
    "def label2onehot(lbl):\n",
    "    # Creates a placeholder of size (10, 60000)\n",
    "    d = np.zeros((lbl.max() + 1, lbl.size))\n",
    "    \n",
    "    # One-hot encode the labels\n",
    "    d[lbl, np.arange(lbl.size)] = 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 60000)\n",
      "0.0 1.0\n",
      "Label at index 42 is 7\n",
      "Corresponding one-hot encodiing is [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "dtrain = label2onehot(ltrain)\n",
    "print(dtrain.shape)\n",
    "print(np.amin(dtrain), np.amax(dtrain))\n",
    "print(\"Label at index 42 is\", ltrain[42])\n",
    "print(\"Corresponding one-hot encodiing is\", dtrain[:, 42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one hot encoding line works as the $1^{st}$ index is traveresed independently of the $2^{nd}$ index<br>\n",
    "So, for each image given by the $2^{nd}$ axis, only the row index given by the value of the label is assigned $1$<br>\n",
    "Thus, $0$ maps to $[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]$ and 9 maps to $[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]$<br><br>\n",
    "We also checked the label for image $42$. The label is $7$ and the corresponding one-hot encoding is $[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot2label(d):\n",
    "    lbl = d.argmax(axis=0)\n",
    "    return lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot answer 7 | Original: 7\n"
     ]
    }
   ],
   "source": [
    "# Checking if this works\n",
    "lab = dtrain[:, 42]\n",
    "che = onehot2label(lab)\n",
    "\n",
    "print(\"One-hot answer\", che, \"| Original:\", ltrain[42])\n",
    "assert(che == ltrain[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have thus checked if our implementation of recovering the label from one-hot encoding is correct<br>\n",
    "The label of the image at index at $42$ is $7$<br>\n",
    "The $onehot2label()$ function recovers this correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the softmax function\n",
    "def softmax(a):\n",
    "    # Calculate the max value\n",
    "    M = np.max(a, axis=0)\n",
    "    \n",
    "    # Subtract for easier exponential calculation\n",
    "    a_m = a - M\n",
    "    \n",
    "    # Calculate the exponent for each class for each image\n",
    "    exp_a_m = np.exp(a_m)\n",
    "    \n",
    "    # Calculate the sum for each image\n",
    "    den = np.sum(exp_a_m, axis=0)\n",
    "    \n",
    "    # Get the probabilities for each class for each image\n",
    "    g_a = exp_a_m / den\n",
    "    # print(np.min(g_a), np.max(g_a))\n",
    "    return g_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to show that $$\\frac{\\partial{g(a)_i}}{\\partial{a_i}} = g(a)_i(1 - g(a)_i)$$<br>\n",
    "By definition above, Softmax is $$y_i = g(a)_i = \\frac{exp(a_i)}{\\sum_{j=1}^{10}exp(a_j)}$$\n",
    "So, $$\\frac{\\partial{g(a)_i}}{\\partial{a_i}} = \\frac{\\partial \\left({\\frac{exp(a_i)}{\\sum_{j=1}^{10}exp(a_j)}} \\right)}{\\partial{a_i}}$$\n",
    "Using the division rule of derivatives, we have $$\\frac{\\partial{g(a)_i}}{\\partial{a_i}} = \\frac{\\sum_{j=1}^{10}exp(a_j)\\frac{\\partial{exp(a_i)}}{\\partial{a_i}} - exp(a_i)\\frac{\\partial \\left( {\\sum_{j=1}^{10}exp(a_j)} \\right)}{\\partial{a_i}}}{\\left( \\sum_{j=1}^{10}exp(a_j) \\right)^2}$$\n",
    "Simplifying, we have $$\\frac{\\partial{g(a)_i}}{\\partial{a_i}} = \\frac{exp(a_i)* \\sum_{j=1}^{10}exp(a_j) - exp(a_i)*exp(a_i)}{\\left( \\sum_{j=1}^{10}exp(a_j) \\right)^2}$$\n",
    "Taking $\\frac{exp(a_i)}{\\sum_{j=1}^{10}exp(a_j)}$ outside, we have $$\\frac{\\partial{g(a)_i}}{\\partial{a_i}} = \\frac{exp(a_i)}{\\sum_{j=1}^{10}exp(a_j)} * \\left( \\frac{\\sum_{j=1}^{10}exp(a_j) - exp(a_i)}{\\left( \\sum_{j=1}^{10}exp(a_j) \\right)} \\right)$$\n",
    "We know that $g(a)_i = \\frac{exp(a_i)}{\\sum_{j=1}^{10}exp(a_j)}$<br>\n",
    "Thus, we have $$\\frac{\\partial{g(a)_i}}{\\partial{a_i}} = g(a)_i * \\left( 1 - g(a)_i \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to show that $$\\frac{\\partial{g(a)_i}}{\\partial{a_j}} = -g(a)_i*g(a)_j for j\\neq i$$<br>\n",
    "By definition above, Softmax is $$y_i = g(a)_i = \\frac{exp(a_i)}{\\sum_{j=1}^{10}exp(a_j)}$$\n",
    "So, $$\\frac{\\partial{g(a)_i}}{\\partial{a_j}} = \\frac{\\partial \\left({\\frac{exp(a_i)}{\\sum_{j=1}^{10}exp(a_j)}} \\right)}{\\partial{a_j}}$$\n",
    "Taking the term $exp(a_i)$ outside, we have, $$\\frac{\\partial{g(a)_i}}{\\partial{a_j}} = exp(a_i) * \\frac{\\partial \\left({\\frac{1}{\\sum_{j=1}^{10}exp(a_j)}} \\right)}{\\partial{a_j}}$$\n",
    "Using inverse rule of derivatives, we have $$\\frac{\\partial{g(a)_i}}{\\partial{a_j}} = exp(a_i) * \\frac{-1*exp(a_j)}{\\left( \\sum_{j=1}^{10}exp(a_j) \\right)^2}$$\n",
    "We know that $g(a)_i = \\frac{exp(a_i)}{\\sum_{j=1}^{10}exp(a_j)}$<br>\n",
    "Thus, we have $$\\frac{\\partial{g(a)_i}}{\\partial{a_j}} = -1*g(a)_i*g(a)_j for j\\neq i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\textbf{We need to prove that the Jacobian of the softmax() function is symmetric}$<br>\n",
    "We have the Jacobian listed as follows, $$ \\frac{\\partial g(a)}{\\partial a} = \\begin{bmatrix}\n",
    "\\frac{\\partial g(a)_1}{\\partial a_1} & \\frac{\\partial g(a)_1}{\\partial a_2} & \\cdots & \\frac{\\partial g(a)_1}{\\partial a_{10}}\\\\\n",
    "\\frac{\\partial g(a)_2}{\\partial a_1} & \\frac{\\partial g(a)_2}{\\partial a_2} & \\cdots & \\frac{\\partial g(a)_2}{\\partial a_{10}}\\\\\n",
    "\\vdots &    &  &  \\vdots \\\\\n",
    "\\frac{\\partial g(a)_{10}}{\\partial a_1} & \\frac{\\partial g(a)_{10}}{\\partial a_2} & \\cdots & \\frac{\\partial g(a)_{10}}{\\partial a_{10}}\\\\\n",
    "\\end{bmatrix}$$\n",
    "From question 9, we know that $$ \\frac{\\partial{g(a)_i}}{\\partial{a_j}} = -1*g(a)_i*g(a)_j for j\\neq i $$\n",
    "From question 8, we have $$ \\frac{\\partial{g(a)_i}}{\\partial{a_i}} = g(a)_i * \\left( 1 - g(a)_i \\right) $$\n",
    "So, $$ \\frac{\\partial g(a)}{\\partial a} = \\begin{bmatrix}\n",
    "g(a)_1(1-g(a)_1) & -g(a)_1g(a)_2 & \\cdots & -g(a)_1g(a)_{10}\\\\\n",
    "-g(a)_2g(a)_1 & g(a)_2(1-g(a)_2) & \\cdots & -g(a)_2g(a)_{10}\\\\\n",
    "\\vdots &    &  & \\vdots \\\\\n",
    "-g(a)_{10}g(a)_1 & -g(a)_{10}g(a)_2 & \\cdots & g(a)_{10}(1-g(a)_{10})\\\\\n",
    "\\end{bmatrix}$$\n",
    "Taking transpose of the Jacobian, we have $$ \\left[ \\frac{\\partial{g(a)}}{\\partial{a}} \\right]^T = \\begin{bmatrix}\n",
    "g(a)_1(1-g(a)_1) & -g(a)_2g(a)_1 & \\cdots & -g(a)_{10}g(a)_{1}\\\\\n",
    "-g(a)_1g(a)_2 & g(a)_2(1-g(a)_2) & \\cdots & -g(a)_{10}g(a)_{2}\\\\\n",
    "\\vdots &    &  & \\vdots \\\\\n",
    "-g(a)_{1}g(a)_{10} & -g(a)_{2}g(a)_{10} & \\cdots & g(a)_{10}(1-g(a)_{10})\\\\\n",
    "\\end{bmatrix}$$\n",
    "Thus, we have proved that $$ \\left[ \\frac{\\partial{g(a)}}{\\partial{a}} \\right]^T = \\frac{\\partial{g(a)}}{\\partial{a}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. $\\textbf{Now, we need to prove } \\delta = g(a) \\otimes e - <g(a),e>g(a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{We know that } \\quad \\delta = \\left[ \\frac{\\partial{g(a)}}{\\partial{a}} \\right]^T \\times e $$\n",
    "$\\textbf{LHS}$\n",
    "$$\\delta = \\left[\\frac{\\partial g(a)}{\\partial a}\\right]^T \\times e$$\n",
    "\n",
    "From above, we know that $$\\left[ \\frac{\\partial{g(a)}}{\\partial{a}} \\right]^T = \\frac{\\partial{g(a)}}{\\partial{a}}$$\n",
    "\n",
    "So,\n",
    "$$ \\delta = \\frac{\\partial g(a)}{\\partial a} \\times e $$\n",
    "\n",
    "$$ \\delta = \\begin{bmatrix}\n",
    "g(a)_1(1-g(a)_1) & -g(a)_1g(a)_2 & \\cdots & -g(a)_1g(a)_{10}\\\\\n",
    "-g(a)_2g(a)_1 & g(a)_2(1-g(a)_2) & \\cdots & -g(a)_2g(a)_{10}\\\\\n",
    "\\vdots &  &  &  \\vdots \\\\\n",
    "-g(a)_{10}g(a)_1 & -g(a)_{10}g(a)_2 & \\cdots & g(a)_{10}(1-g(a)_{10})\\\\\n",
    "\\end{bmatrix} \\times e $$\n",
    "\n",
    "$$ \\text{where} \\quad e = \\begin{pmatrix}\n",
    "e_1\\\\e_2\\\\\\vdots\\\\e_{10}\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "$$ \\text{So,} \\quad \\delta = \\begin{bmatrix}\n",
    "g(a)_1(1-g(a)_1) & -g(a)_1g(a)_2 & \\cdots & -g(a)_1g(a)_{10}\\\\\n",
    "-g(a)_2g(a)_1 & g(a)_2(1-g(a)_2) & \\cdots & -g(a)_2g(a)_{10}\\\\\n",
    "\\vdots &  &  &  \\vdots \\\\\n",
    "-g(a)_{10}g(a)_1 & -g(a)_{10}g(a)_2 & \\cdots & g(a)_{10}(1-g(a)_{10})\\\\\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{pmatrix}\n",
    "e_1\\\\e_2\\\\\\vdots\\\\e_{10}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "$$\\implies \\delta = \\begin{bmatrix}\n",
    "g(a)_1(1-g(a)_1)e_1 + -g(a)_1g(a)_2e_2 + \\cdots + -g(a)_1g(a)_{10}e_{10}\\\\\n",
    "-g(a)_2g(a)_1e_1 + g(a)_2(1-g(a)_2)e_2 + \\cdots + -g(a)_2g(a)_{10}e_{10}\\\\\n",
    "\\vdots \\\\\n",
    "-g(a)_{10}g(a)_1e_1 + -g(a)_{10}g(a)_2e_2 + \\cdots + g(a)_{10}(1-g(a)_{10})e_{10}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$\\text{So,} \\quad \\delta = \\begin{bmatrix}\n",
    "g(a)_1\\left[e_1-g(a)_1e_1 + -g(a)_2e_2 + \\cdots + -g(a)_{10}e_{10}\\right]\\\\\n",
    "g(a)_2\\left[e_2 - g(a)_1e_1 + -g(a)_2e_2 + \\cdots + -g(a)_{10}e_{10}\\right]\\\\\n",
    "\\vdots \\\\\n",
    "g(a)_{10}\\left[e_{10} - g(a)_1e_1 + -g(a)_2e_2 + \\cdots + -g(a)_{10}e_{10}\\right]\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$ \\text{So,} \\quad \\delta = \\begin{bmatrix}\n",
    "g(a)_1\\left[e_1 - \\sum_{i=1}^{10}g(a)_ie_i\\right]\\\\\n",
    "g(a)_2\\left[e_2 - \\sum_{i=1}^{10}g(a)_ie_i\\right]\\\\\n",
    "\\vdots \\\\\n",
    "g(a)_{10}\\left[e_{10} - \\sum_{i=1}^{10}g(a)_ie_i\\right]\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{RHS}$\n",
    "$$g(a) \\otimes e - <g(a),e>g(a)$$\n",
    "Individually, these terms are as follows\n",
    "$$ g(a) \\otimes e = \\begin{bmatrix}\n",
    "g(a)_1.e_i \\\\\n",
    "g(a)_2.e_2 \\\\\n",
    "\\vdots \\\\\n",
    "g(a)_{10}.e_{10}\n",
    "\\end{bmatrix} $$\n",
    "$$ <g(a),e> = \\left( \\sum_{i=1}^{10}g(a)_ie_i \\right)$$\n",
    "Plugging these in RHS, we get\n",
    "$$ g(a) \\otimes e - <g(a),e>g(a) = \\begin{bmatrix}\n",
    "g(a)_1.e_1 \\\\\n",
    "g(a)_2.e_2 \\\\\n",
    "\\vdots \\\\\n",
    "g(a)_{10}.e_{10}\n",
    "\\end{bmatrix} - \\left( \\sum_{i=1}^{10}g(a)_ie_i \\right)\\begin{bmatrix}\n",
    "g(a)_1 \\\\\n",
    "g(a)_2 \\\\\n",
    "\\vdots \\\\\n",
    "g(a)_{10}\n",
    "\\end{bmatrix}$$\n",
    "Thus, we have\n",
    "$$ g(a) \\otimes e - <g(a),e>g(a) = \\begin{bmatrix}\n",
    "g(a)_1.e_1 \\\\\n",
    "g(a)_2.e_2 \\\\\n",
    "\\vdots \\\\\n",
    "g(a)_{10}.e_{10}\n",
    "\\end{bmatrix} - \\begin{bmatrix}\n",
    "g(a)_1 \\sum_{i=1}^{10}g(a)_ie_i \\\\\n",
    "g(a)_2 \\sum_{i=1}^{10}g(a)_ie_i\\\\\n",
    "\\vdots \\\\\n",
    "g(a)_{10} \\sum_{i=1}^{10}g(a)_ie_i\n",
    "\\end{bmatrix}$$\n",
    "So, finally we have\n",
    "$$ g(a) \\otimes e - <g(a),e>g(a) = \\begin{bmatrix}\n",
    "g(a)_1\\left[e_1 - \\sum_{i=1}^{10}g(a)_ie_i\\right]\\\\\n",
    "g(a)_2\\left[e_2 - \\sum_{i=1}^{10}g(a)_ie_i\\right]\\\\\n",
    "\\vdots \\\\\n",
    "g(a)_{10}\\left[e_{10} - \\sum_{i=1}^{10}g(a)_ie_i\\right]\n",
    "\\end{bmatrix}$$\n",
    "We notice that LHS = RHS<br>\n",
    "Hence, we have proved that $$ \\delta = g(a) \\otimes e - <g(a),e>g(a) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the gradient of the softmax function\n",
    "# The directional derivative of the softmax function is as follows:-\n",
    "# delta = elementwise product (g(a) and e) - <g(a),e> g(a)\n",
    "def softmaxp(a, e):\n",
    "    # Calculate g(a)\n",
    "    g_a = softmax(a)\n",
    "    \n",
    "    # Calculate term 1\n",
    "    t1 = g_a * e\n",
    "    \n",
    "    # Calculate the directional derivative\n",
    "    delta = t1 - np.sum(t1, axis=0)*g_a\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.882725480374909e-07 should be smaller than 1e-6\n"
     ]
    }
   ],
   "source": [
    "# Check if softmaxp is correct\n",
    "# finite difference step\n",
    "eps = 1e-6\n",
    "\n",
    "# random inputs\n",
    "a = np.random.randn(10, 200)\n",
    "\n",
    "# random directions\n",
    "e = np.random.randn(10, 200)\n",
    "\n",
    "# testing part\n",
    "diff = softmaxp(a, e)\n",
    "\n",
    "# From the definition of a derivative, we have\n",
    "diff_approx = (softmax(a + eps*e) - softmax(a)) / eps\n",
    "\n",
    "# Calculate the relative error of these 2 approaches\n",
    "rel_error = np.abs(diff - diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "\n",
    "# print the relative error\n",
    "print(rel_error, 'should be smaller than 1e-6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented the code to compute the directional derivative of $g$ at point $a$ in the direction of $e$ using the $softmaxp(a, e)$ function<br>\n",
    "We tested the implementation of our code by comparing with the fundamental definition of directional derivative, where, $$ \\delta = \\frac{\\partial g(a)}{\\partial a} \\times e = \\lim_{\\varepsilon\\to0} \\frac{g(a + \\varepsilon e) - g(a)}{\\varepsilon} $$\n",
    "We verified that our implementation of $softmaxp()$ is correct and that the relative error is smaller that $1e-6$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Computing the derivative of relu()}$<br>\n",
    "We know that for ReLU, $g(a)_i = max(a_i, 0)$<br>\n",
    "Since for ReLU the activation is element-wise, i.e., $g(a)_i = g(a_i)$, the Jacobian is diagonal<br>\n",
    "Hence, $$ \\delta = \\frac{\\partial g(a)}{\\partial a} \\otimes e $$\n",
    "$$ \\implies \\delta = g'(a) \\otimes e $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the ReLU(a) = max(ai, 0)\n",
    "def relu(a):\n",
    "    # Create a copy of the array a\n",
    "    g_a = np.copy(a)\n",
    "    \n",
    "    # Set those values less than 0 to 0\n",
    "    g_a[a < 0] = 0\n",
    "    return g_a\n",
    "\n",
    "def relup(a, e):\n",
    "    # Relup is the directional derivative of ReLU(a) in the direction of e\n",
    "    # Taking the Jacobian for ReLU and then deriving, we have found that the derivative is as given:-\n",
    "    # It is the element-wise product of gradient of relu and the vector e\n",
    "    # Create a copy of the array a\n",
    "    del_a = np.copy(a)\n",
    "    \n",
    "    # Set the values less than 0 to 0\n",
    "    del_a[a < 0] = 0\n",
    "    \n",
    "    # Set the values greater than 0 to 1\n",
    "    del_a[a > 0] = 1\n",
    "    \n",
    "    # Compute delta as the element-wise product of the gradient of relu and the vector e\n",
    "    delta = del_a * e\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented the relu function and its directional derivative now<br>\n",
    "We used the Jacobian to derive the relation of relup to vector operations<br>\n",
    "We shall now test $reulp()$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3060646935183765e-11 should be smaller than 1e-6\n"
     ]
    }
   ],
   "source": [
    "# Check if relup is correct\n",
    "# finite difference step\n",
    "eps = 1e-6\n",
    "\n",
    "# random inputs\n",
    "a = np.random.randn(10, 200)\n",
    "\n",
    "# random directions\n",
    "e = np.random.randn(10, 200)\n",
    "\n",
    "# testing part\n",
    "diff = relup(a, e)\n",
    "\n",
    "# From the definition of a derivative, we have\n",
    "diff_approx = (relu(a + eps*e) - relu(a)) / eps\n",
    "\n",
    "# Calculate the relative error of these 2 approaches\n",
    "rel_error = np.abs(diff - diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "\n",
    "# print the relative error\n",
    "print(rel_error, 'should be smaller than 1e-6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented the code to compute the directional derivative of $g$ at point $a$ in the direction of $e$ using the $relup(a, e)$ function<br>\n",
    "We tested the implementation of our code by comparing with the fundamental definition of directional derivative, where, $$ \\delta = \\frac{\\partial g(a)}{\\partial a} \\times e = \\lim_{\\varepsilon\\to0} \\frac{g(a + \\varepsilon e) - g(a)}{\\varepsilon} $$\n",
    "We verified that our implementation of $relup()$ is correct and that the relative error is smaller that $1e-6$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and initialize our shallow network\n",
    "def init_shallow(Ni, Nh, No):\n",
    "    \"\"\"\n",
    "    Ni - dimension of the input layer. Ni = 784\n",
    "    Nh - dimension of the hidden layer. Nh = 64\n",
    "    No - dimension of the output layer. No = 10\n",
    "    \"\"\"\n",
    "    # Create the bias vector for the 1st layer\n",
    "    # We are using He initialization method\n",
    "    b1 = np.random.randn(Nh, 1) / np.sqrt((Ni + 1.) / 2.)\n",
    "    # Create the synaptic weights between the input and the hidden neurons\n",
    "    W1 = np.random.randn(Nh, Ni) / np.sqrt((Ni + 1.) / 2.)\n",
    "    \n",
    "    # Create the bias vector for the 2nd layer\n",
    "    # We are using Xavier initialization method\n",
    "    b2 = np.random.randn(No, 1) / np.sqrt((Nh + 1.))\n",
    "    # Create the synaptic weights between the hidden and the output neurons\n",
    "    W2 = np.random.randn(No, Nh) / np.sqrt((Nh + 1.))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Initialize our shallow network\n",
    "Ni = norm_x_train.shape[0]\n",
    "Nh = 64\n",
    "No = dtrain.shape[0]\n",
    "netinit = init_shallow(Ni, Nh, No)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined the network architecture and parameters and initialized them in the snippet above<br>\n",
    "We used He initialization for the input neurons to hidden neurons connections, and we used Xavier initialization for the hidden neurons to output neurons connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the forward_prop function to propagate the activations through the network\n",
    "def forwardprop_shallow(x, net):\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    \n",
    "    # Input to hidden neurons\n",
    "    a1 = W1.dot(x) + b1\n",
    "    h1 = relu(a1)\n",
    "    \n",
    "    # Hidden to output neurons\n",
    "    a2 = W2.dot(h1) + b2\n",
    "    y = softmax(a2)\n",
    "    return y\n",
    "\n",
    "# Calculate the initial output for the random initializations\n",
    "yinit = forwardprop_shallow(norm_x_train, netinit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n",
      "(10, 60000)\n",
      "-1.0 1.0\n",
      "0.0018577635031450167 0.8057231387657491\n"
     ]
    }
   ],
   "source": [
    "print(norm_x_train.shape)\n",
    "print(yinit.shape)\n",
    "\n",
    "print(np.min(norm_x_train), np.max(norm_x_train))\n",
    "print(np.min(yinit), np.max(yinit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented the function to propagate forward through the network<br>\n",
    "We subsequently calculated the initial output for our initialization of the network with random parameter values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2638596794329442 should be around .26\n"
     ]
    }
   ],
   "source": [
    "# Function to compute the cross-entropy loss\n",
    "def eval_loss(y, d):\n",
    "    # Calculates the log of the predicted probabilities\n",
    "    log_y = np.log(y)\n",
    "    \n",
    "    # Element-wise multiplication with d\n",
    "    mult = d*log_y\n",
    "    \n",
    "    # Take the negative to get cross-entropy\n",
    "    mult = -1 * mult\n",
    "    \n",
    "    # calculate the sum over all probabilities and sum over all the input vectors\n",
    "    sum_pro = np.sum(mult)\n",
    "    \n",
    "    # Calculate the average of the cross-entropy\n",
    "    ret = np.mean(mult)\n",
    "    return ret\n",
    "\n",
    "# Check the evaluation loss for the initial predictions\n",
    "print(eval_loss(yinit, dtrain), 'should be around .26')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have thus implemented the function to calculate the loss<br>\n",
    "We have verified that the initial loss is around .26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.175 % of the images are misclassified\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate the percentage\n",
    "def eval_perfs(y, lbl):\n",
    "    # Convert the given probabilities to corresponding label\n",
    "    pred_lbl = onehot2label(y)\n",
    "    \n",
    "    # Compare the groundtruth with the predicted label and identify Misclassified samples\n",
    "    comps = [pred_lbl != lbl]\n",
    "    nums = np.sum(comps)\n",
    "    ret = (nums * 1.0 / lbl.shape[0]) * 100.0\n",
    "    return ret\n",
    "\n",
    "# Print the percentage of \"mis-classified\" samples for the initial predicted probabilities\n",
    "# and the groundtruth labels\n",
    "print(eval_perfs(yinit, ltrain), \"% of the images are misclassified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented the function to calculate the percentage of mis-classified samples<br>\n",
    "We picked the index with the maximum value(probability) for each column using the $y.argmax(axis=0)$ function<br>\n",
    "This index is basically the predicted class of the given image(column)<br>\n",
    "We then compared the predicted label with the actual label, calculated the number of misclassified images, and then divided by the total number of images to get the percentage of mis-classification<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to show that $$ \\left( \\nabla_yE \\right)_i =  -\\frac{d_i}{y_i} $$\n",
    "$E$ is cross-entropy loss, and is given by $$ E = - \\sum_{i=1}^{10} d_{i}log(y_{i}) $$\n",
    "Differentiating with respect to $y_i$ and taking $-d_i$ as common, we have $$ \\left( \\nabla_yE \\right)_i = -d_i * \\frac{\\partial \\left( \\sum_{i=1}^{10}log(y_i) \\right)}{\\partial y_i} $$\n",
    "Thus, we have $$ \\left( \\nabla_yE \\right)_i = -d_i * \\left( \\frac{1}{y_i} \\right) $$\n",
    "Hence, proved that $$ \\left( \\nabla_yE \\right)_i =  -\\frac{d_i}{y_i} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform backpropagation in the network\n",
    "def update_shallow(x, d, net, gamma=.05):\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    \n",
    "    Ni = W1.shape[1]\n",
    "    Nh = W1.shape[0]\n",
    "    No = W2.shape[0]\n",
    "    \n",
    "    # Normalize the gamma by the training dataset size\n",
    "    gamma = gamma / x.shape[1]\n",
    "    \n",
    "    ## Backprop begins!\n",
    "    # Forward prop through the network using current parameters\n",
    "    # This calculates the predicted probabilities for each class\n",
    "    # working dim - 64*60000 - h1; 10*60000 - y\n",
    "    a1 = W1.dot(x) + b1\n",
    "    h1 = relu(a1)\n",
    "    \n",
    "    # Hidden to output neurons\n",
    "    a2 = W2.dot(h1) + b2\n",
    "    y = softmax(a2)\n",
    "    \n",
    "    # Calculate the loss\n",
    "    # e = eval_loss(y_pred, dtrain)\n",
    "    \n",
    "    ## Backprop through output neurons to hidden neurons\n",
    "    # Calculate the gradient of the error for output neurons\n",
    "    # working dim - 10*60000\n",
    "    # DEBUG\n",
    "    #print(d.shape, y_pred.shape)\n",
    "    #print(np.min(d), np.max(d))\n",
    "    #print(np.min(y_pred), np.max(y_pred))\n",
    "    e2 = -1.0 * d / y\n",
    "    \n",
    "    # calculate derivative of softmax() activation\n",
    "    # working dim - 10*60000\n",
    "    #delta2 = softmaxp(y_pred, e2)\n",
    "    #delta2 = softmaxp(y, e2)\n",
    "    delta2 = softmaxp(a2, e2)\n",
    "    \n",
    "    # Calculate the derivative of E wrt W2\n",
    "    # working dim - 10*60000 * 60000*64(h1.T)\n",
    "    grad_w2_e = delta2.dot(h1.T)\n",
    "    \n",
    "    # Calculate the derivative of E wrt b2\n",
    "    # working dim - 10*60000 * 60000*1 = 10*1\n",
    "    grad_b2_e = delta2.dot(np.ones((delta2.shape[1], 1)))\n",
    "    \n",
    "    # Calculate the gradient of the error for the hidden neurons\n",
    "    # Working dim - 64*60000\n",
    "    # 64*10(W2 is 10*64) * 10*60000\n",
    "    e1 = W2.T.dot(delta2)\n",
    "    \n",
    "    # Calculate the derivative of the relu() activation\n",
    "    # working dim - 64*60000\n",
    "    #delta1 = relup(h1, e1)\n",
    "    delta1 = relup(a1, e1)\n",
    "    \n",
    "    # Calculate the derivative of E wrt W1\n",
    "    # working dim - 64*60000 * 60000*784(x.T) (h0 = x)\n",
    "    grad_w1_e = delta1.dot(x.T)\n",
    "    \n",
    "    # Calculate the derivative of E wrt b1\n",
    "    # working dim - 64*60000 * 60000*1 = 64*1\n",
    "    grad_b1_e = delta1.dot(np.ones((delta1.shape[1], 1)))\n",
    "    \n",
    "    ## UPDATE the parameters\n",
    "    W2 = W2 - gamma * grad_w2_e\n",
    "    W1 = W1 - gamma * grad_w1_e\n",
    "    b2 = b2 - gamma * grad_b2_e\n",
    "    b1 = b1 - gamma * grad_b1_e\n",
    "    \n",
    "    # return the updated parameters\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we have written the function to perform one backpropagation update for our shallow network.\n",
    "We have also proved that $$ \\left( \\nabla_yE \\right)_i =  -\\frac{d_i}{y_i} $$\n",
    "We then used $softmaxp()$ and $relup()$ to calculate the gradients<br>\n",
    "We implemented the backpropagation layer-wise from the output neurons to the hidden neurons.<br>\n",
    "We coded the backpropagation as given:-\n",
    "$$ W_k^{t+1} = W_k^t - \\gamma \\nabla_{w_k}E^t $$\n",
    "$$ b_k^{t+1} = b_k^t - \\gamma \\nabla_{b_k}E^t $$\n",
    "$$ where \\quad \\nabla_{w_k}E = \\delta_k h_{k-1}^T $$\n",
    "$$ and \\quad \\nabla_{b_k}E = \\delta_k 1_N $$\n",
    "$$ and \\quad \\delta_k = \\left[ \\frac{\\partial g_k(a_k)}{\\partial a_k} \\right]^T \\times e_k $$\n",
    "$$ where \\quad e_k = \\left\\{ \\begin{array}{ll} \\nabla_y E \\quad \\text{if k is an output layer} \\\\ W_{k+1}^T \\delta_{k+1} \\quad \\text{otherwise} \\end{array} \\right.$$\n",
    "We finally return the network parameters $W_1, b_1, W_2 and b_2$ after one iteration of backpropagation to the caller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute backprop_shallow\n",
    "def backprop_shallow(x, d, net, T, gamma=.05):\n",
    "    # Get the label given the one-hot encoding\n",
    "    lbl = onehot2label(d)\n",
    "    \n",
    "    # Compute and display the loss and performance measure initially\n",
    "    y = forwardprop_shallow(x, net)\n",
    "    tr_loss = eval_loss(y, d)\n",
    "    print(\"Initial loss is:\", tr_loss)\n",
    "    tr_perf = eval_perfs(y, lbl)\n",
    "    print(tr_perf, \"% of images are misclassified initially\\n\")\n",
    "    \n",
    "    for t in range(T):\n",
    "        # update the parameters using the update_shallow() function\n",
    "        net = update_shallow(x, d, net, gamma)\n",
    "        \n",
    "        # Compute and display the loss and performance measure for every 10 iterations\n",
    "        if t%10 == 9:\n",
    "            y = forwardprop_shallow(x, net)\n",
    "            tr_loss = eval_loss(y, d)\n",
    "            print(\"Training loss after iteration\", t+1, \"is:\", tr_loss)\n",
    "            tr_perf = eval_perfs(y, lbl)\n",
    "            print(tr_perf, \"% of images are misclassified after iteration\", t+1,\"\\n\")\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss is: 0.2638596794329442\n",
      "91.175 % of images are misclassified initially\n",
      "\n",
      "Training loss after iteration 10 is: 0.16191209907405543\n",
      "42.126666666666665 % of images are misclassified after iteration 10 \n",
      "\n",
      "Training loss after iteration 20 is: 0.11597192658503634\n",
      "28.14833333333333 % of images are misclassified after iteration 20 \n",
      "\n",
      "Training loss after iteration 30 is: 0.09096238435774094\n",
      "22.475 % of images are misclassified after iteration 30 \n",
      "\n",
      "Training loss after iteration 40 is: 0.08444967562032382\n",
      "26.521666666666665 % of images are misclassified after iteration 40 \n",
      "\n",
      "Training loss after iteration 50 is: 0.07062631389995717\n",
      "20.724999999999998 % of images are misclassified after iteration 50 \n",
      "\n",
      "Training loss after iteration 60 is: 0.061937629673339566\n",
      "16.71333333333333 % of images are misclassified after iteration 60 \n",
      "\n",
      "Training loss after iteration 70 is: 0.05701452523889525\n",
      "15.273333333333333 % of images are misclassified after iteration 70 \n",
      "\n",
      "Training loss after iteration 80 is: 0.05346541739673409\n",
      "14.456666666666667 % of images are misclassified after iteration 80 \n",
      "\n",
      "Training loss after iteration 90 is: 0.05070839570076598\n",
      "13.766666666666666 % of images are misclassified after iteration 90 \n",
      "\n",
      "Training loss after iteration 100 is: 0.048489191601058614\n",
      "13.243333333333332 % of images are misclassified after iteration 100 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the net for 2 iterations initially. The output is the final parameters after training\n",
    "# Now training net for 100 iterations\n",
    "nettrain = backprop_shallow(norm_x_train, dtrain, netinit, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrote the code for $backprop\\_shallow()$ to train the network<br>\n",
    "This function performs $T$ updates of the network by calling one instance of the backpropagation function $update\\_shallow()$ each time<br>\n",
    "We evaluate the loss initially and after each iteration by calling the $eval_loss()$ function and then display it<br>\n",
    "Similarly, we evaluate the percentage of images mis-classified initially and after each iteration by calling the $eval_perfs()$ function and then display it<br>\n",
    "This function finally returns the completely trained parameters $W_1, b_1, W_2, b_2$ after $T$ iterations and stores it in $nettrain$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\textbf{2 iterations}$$\n",
    "We tried running  the code with $2$ iterations initially.<br>\n",
    "$2$ iterations worked in reducing the loss from $0.255$ to $0.219$<br>\n",
    "It also reduced the percentage of misclassified images from $91.41\\%$ to $80.23\\%$<br>\n",
    "So we moved onto testing the function with $5$ iterations<br><br>\n",
    "$$\\textbf{5 iterations}$$\n",
    "$5$ iterations worked in reducing the loss from $0.264$ to $0.183$<br>\n",
    "It also reduced the percentage of misclassified images from $89.82\\%$ to $49.83\\%$<br>\n",
    "So, we moved onto testing the function with $20$ iterations<br><br>\n",
    "$$\\textbf{20 iterations}$$\n",
    "$20$ iterations worked in reducing the loss from $0.270$ to $0.127$<br>\n",
    "It also reduced the percentage of misclassified images from $91.122\\%$ to $34.36\\%$<br>\n",
    "We finally run the network with $100$ iterations<br><br>\n",
    "$$\\textbf{100 iterations}$$\n",
    "$100$ iterations worked in reducing the loss from $0.264$ to $0.048$<br>\n",
    "It also reduced the percentage of misclassified images from $91.175\\%$ to $13.243\\%$<br>\n",
    "<b>This result is also consistent with the value given in the question</b>. We indeed reached about $13\\%$ of training errors with $T = 100$ iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10000)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the testing data\n",
    "xtest, ltest = MNISTtools.load(dataset='testing', path='./datasets/MNIST')\n",
    "print(xtest.shape)\n",
    "print(ltest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the testing set of images is found to be $(784, 10000)$, that is, it contains $10000$ images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Displaying the image at index 42\n",
    "MNISTtools.show(xtest[:, 42])\n",
    "\n",
    "# Print its corresponding label\n",
    "print(ltest[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of xtest is from  0  to  255\n",
      "Data type of xtest is  uint8\n"
     ]
    }
   ],
   "source": [
    "# Find the range and type of xtest\n",
    "min_te_x = np.amin(xtest)\n",
    "max_te_x = np.amax(xtest)\n",
    "\n",
    "print(\"Range of xtest is from \", min_te_x, \" to \", max_te_x)\n",
    "print(\"Data type of xtest is \", xtest.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10000)\n",
      "Range of normalized xtest is -1.0 to 1.0\n",
      "Data type of normalized xtest is float32\n"
     ]
    }
   ],
   "source": [
    "# Normalize the test images\n",
    "norm_x_test = normalize_MNIST_images(xtest)\n",
    "print(norm_x_test.shape)\n",
    "print(\"Range of normalized xtest is\", np.amin(norm_x_test), \"to\", np.amax(norm_x_test))\n",
    "print(\"Data type of normalized xtest is\", norm_x_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10000)\n",
      "0.0 1.0\n",
      "Label at index 42 is 4\n",
      "Corresponding one-hot encodiing is [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "dtest = label2onehot(ltest)\n",
    "print(dtest.shape)\n",
    "print(np.amin(dtest), np.amax(dtest))\n",
    "print(\"Label at index 42 is\", ltest[42])\n",
    "print(\"Corresponding one-hot encodiing is\", dtest[:, 42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss is: 0.04600023010648992\n",
      "12.24 % of images are misclassified in the test set\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute and display the loss and performance measure on the test set\n",
    "y_test = forwardprop_shallow(norm_x_test, nettrain)\n",
    "te_loss = eval_loss(y_test, dtest)\n",
    "print(\"Test loss is:\", te_loss)\n",
    "te_lbl = onehot2label(dtest)\n",
    "te_perf = eval_perfs(y_test, te_lbl)\n",
    "print(te_perf, \"% of images are misclassified in the test set\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we have loaded the testing dataset<br>\n",
    "$$\\textbf{2 iterations testing vs training}$$\n",
    "We tested the performance of the network parameters that were trained for $2$ iterations and we got a loss of $0.218$ on this test set<br>\n",
    "Training loss after $2$ iterations was: $0.219$<br>\n",
    "$79.28\\%$ of the images are mis-classified from the test set after the training<br>\n",
    "$80.23\\%$ of images are misclassified in the training set after $2$ iterations<br><br>\n",
    "$$\\textbf{5 iterations testing vs training}$$\n",
    "We tested the performance of the network parameters that were trained for $5$ iterations and we got a loss of $0.181$ on this test set<br>\n",
    "Training loss after $5$ iterations was: $0.183$<br>\n",
    "$48.14\\%$ of the images are mis-classified from the test set after the training<br>\n",
    "$49.83\\%$ of images are misclassified in the training set after $5$ iterations<br><br>\n",
    "$$\\textbf{20 iterations testing vs training}$$\n",
    "We tested the performance of the network parameters that were trained for $20$ iterations and we got a loss of $0.124$ on this test set<br>\n",
    "Training loss after $20$ iterations was: $0.127$<br>\n",
    "$33.42\\%$ of the images are mis-classified from the test set after the training<br>\n",
    "$34.36\\%$ of images are misclassified in the training set after $20$ iterations<br><br>\n",
    "$$\\textbf{100 iterations testing vs training}$$\n",
    "We tested the performance of the network parameters that were trained for $100$ iterations and we got a loss of $0.046$ on this test set<br>\n",
    "Training loss after $100$ iterations was: $0.048$<br>\n",
    "$12.24\\%$ of the images are mis-classified from the test set after the training<br>\n",
    "$13.243\\%$ of images are misclassified in the training set after $100$ iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backprop using minibatch\n",
    "def backprop_minibatch_shallow(x, d, net, T, B=100, gamma=0.05):\n",
    "    # Get the number of images\n",
    "    N = x.shape[1]\n",
    "    \n",
    "    # Calculate the number of batches\n",
    "    NB = int((N+B-1)/B)\n",
    "    \n",
    "    # Convert one-hot encoded data to a label\n",
    "    lbl = onehot2label(d)\n",
    "    \n",
    "    # Compute and display the loss and performance measure initially\n",
    "    y = forwardprop_shallow(x, net)\n",
    "    tr_mini_loss = eval_loss(y, d)\n",
    "    print(\"Initial minibatch loss is:\", tr_mini_loss)\n",
    "    tr_mini_perf = eval_perfs(y, lbl)\n",
    "    print(tr_mini_perf, \"% of images are misclassified initially using minibatch method\\n\")\n",
    "    \n",
    "    # For every iteration(epoch)\n",
    "    for t in range(T):\n",
    "        # shuffle the indices to access the data\n",
    "        shuffled_indices = np.random.permutation(range(N))\n",
    "        \n",
    "        # For each minibatch\n",
    "        for l in range(NB):\n",
    "            # get the shuffled indices for a given minibatch\n",
    "            minibatch_indices = shuffled_indices[B*l:min(B*(l+1), N)]\n",
    "            # Backprop through the minibatch and update the parameters of the network\n",
    "            net = update_shallow(x[:, minibatch_indices], d[:, minibatch_indices], net, gamma)\n",
    "            \n",
    "        y = forwardprop_shallow(x, net)\n",
    "        tr_mini_loss = eval_loss(y, d)\n",
    "        print(\"Training loss using minibatches after epoch\", t+1, \"is:\", tr_mini_loss)\n",
    "        tr_mini_perf = eval_perfs(y, lbl)\n",
    "        print(tr_mini_perf, \"% of images are misclassified using minibatches after epoch\", t+1,\"\\n\")\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrote the code for $backprop\\_minibatch\\_shallow()$ to train the network<br>\n",
    "In minibatch backpropagation, we divided the dataset into a number of mini-batches, each sized $100$ images<br>\n",
    "We thus updated the parameters of our network TN/B times<br>\n",
    "We first calculate the number of batches to train for<br>\n",
    "We then shuffle the entire dataset, and for each minibatch, we update the parameters of the network using $update_shallow()$ function<br>\n",
    "We evaluate the loss initially and after each epoch by calling the $eval\\_loss()$ function and then display it<br>\n",
    "Similarly, we evaluate the percentage of images mis-classified initially and after each epoch by calling the $eval\\_perfs()$ function and then display it<br>\n",
    "This function finally returns the completely trained parameters $W_1, b_1, W_2, b_2$ after $T$ epochs and stores it in $netminibatch$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0 1.0\n",
      "Initial minibatch loss is: 0.2638596794329442\n",
      "91.175 % of images are misclassified initially using minibatch method\n",
      "\n",
      "Training loss using minibatches after epoch 1 is: 0.03001518878674801\n",
      "8.746666666666666 % of images are misclassified using minibatches after epoch 1 \n",
      "\n",
      "Training loss using minibatches after epoch 2 is: 0.024412366297609372\n",
      "7.1499999999999995 % of images are misclassified using minibatches after epoch 2 \n",
      "\n",
      "Training loss using minibatches after epoch 3 is: 0.020629920459641838\n",
      "6.0183333333333335 % of images are misclassified using minibatches after epoch 3 \n",
      "\n",
      "Training loss using minibatches after epoch 4 is: 0.01799265925320068\n",
      "5.258333333333334 % of images are misclassified using minibatches after epoch 4 \n",
      "\n",
      "Training loss using minibatches after epoch 5 is: 0.01602968399180054\n",
      "4.575 % of images are misclassified using minibatches after epoch 5 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the network for a few epochs\n",
    "print(np.min(norm_x_train), np.max(norm_x_train))\n",
    "netminibatch = backprop_minibatch_shallow(norm_x_train, dtrain, netinit, 5, B=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\textbf{2 epochs minibatch}$$\n",
    "We tried running  the code with $2$ epochs initially.<br>\n",
    "$2$ epochs using minibatches worked in reducing the loss from $0.255$ to $0.025$<br>\n",
    "Using minibatches also reduced the percentage of misclassified images from $91.41\\%$ to $7.145\\%$\n",
    "\n",
    "$$\\textbf{Without Minibatch - 2 iterations}$$\n",
    "This is different from running with $2$ iterations over the whole training set<br>\n",
    "$2$ iterations worked in reducing the loss from $0.255$ to $0.219$<br>\n",
    "It also reduced the percentage of misclassified images from $91.41\\%$ to $80.23\\%$<br>\n",
    "\n",
    "$$\\textbf{5 epochs minibatch}$$\n",
    "We tried running  the code with $5$ epochs after this.<br>\n",
    "$5$ epochs using minibatches worked in reducing the loss from $0.264$ to $0.015$<br>\n",
    "Using minibatches also reduced the percentage of misclassified images from $89.82\\%$ to $4.6\\%$\n",
    "\n",
    "$$\\textbf{Without Minibatch - 5 iterations}$$\n",
    "This is different from running with $5$ iterations over the whole training set<br>\n",
    "$5$ iterations worked in reducing the loss from $0.264$ to $0.183$<br>\n",
    "It also reduced the percentage of misclassified images from $89.82\\%$ to $49.83\\%$<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss after minibatch gradient descent is: 0.016790431647965853\n",
      "4.9799999999999995 % of images are misclassified in the test set after minibatch gradient descent\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute and display the loss and performance measure on the test set\n",
    "y_mini_test = forwardprop_shallow(norm_x_test, netminibatch)\n",
    "te_mini_loss = eval_loss(y_mini_test, dtest)\n",
    "print(\"Test loss after minibatch gradient descent is:\", te_mini_loss)\n",
    "te_mini_perf = eval_perfs(y_mini_test, te_lbl)\n",
    "print(te_mini_perf, \"% of images are misclassified in the test set after minibatch gradient descent\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\textbf{2 epochs minibatch - Test vs. Train} $$\n",
    "We tested the performance of the network parameters that were trained for $2$ epochs and we got a loss of $0.025$ on this test set<br>\n",
    "Training loss after $2$ epochs was: $0.025$<br>\n",
    "$6.99\\%$ of the images are mis-classified from the test set after the minibatch training<br>\n",
    "$7.145\\%$ of images are misclassified in the training set after $5$ epochs<br>\n",
    "\n",
    "$$ \\textbf{2 iterations without minibatch testing set vs. 2 epochs minibatch testing set} $$\n",
    "Testing loss after training without minibatches for $2$ iterations was : $0.218$<br>\n",
    "$79.28\\%$ of the images are mis-classified from the test set without minibatch training<br><br>\n",
    "\n",
    "$$ \\textbf{5 epochs minibatch - Test vs. Train} $$\n",
    "We tested the performance of the network parameters that were trained for $5$ epochs and we got a loss of $0.016$ on this test set<br>\n",
    "Training loss after $5$ epochs was: $0.015$<br>\n",
    "$4.82\\%$ of the images are mis-classified from the test set after the minibatch training<br>\n",
    "$4.6\\%$ of images are misclassified in the training set after $5$ epochs<br>\n",
    "\n",
    "$$ \\textbf{5 iterations without minibatch testing set vs. 5 epochs minibatch testing set} $$\n",
    "Testing loss after training without minibatches for $5$ iterations was : $0.181$<br>\n",
    "$48.14\\%$ of the images are mis-classified from the test set without minibatch training<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "After training for $100$ iterations over the entire dataset, our network had a training loss of $0.048$ and testing set loss of $0.046$. This is higher compared to training the network parameters with minibatches for just $5$ epochs, where the training loss was $0.015$ and the testing set loss was $0.016$<br>\n",
    "The percentage of mis-classified samples follows the same trend.<br>\n",
    "After training for $100$ iterations over the entire dataset, our network had mis-classification percentage as $13.243\\%$ on the training set and $12.24\\%$ on the testing set.<br>\n",
    "This again is a higher error percentage for mis-classified images compared to training the network parameters on random minibatches for just $5$ epochs<br>\n",
    "With just $5$ epochs of training on minibatches, our network had mis-classification percentage as $4.6\\%$ on the training set and $4.82\\%$ on the testing set, which is better than the results we obtained after training the network parameters for $100$ iterations on the entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferences\n",
    "Comparing the performance of the network using minbatches vs. not using it, we conclude that training using minibatch gradient descent gives improved performance compared to training on the entire dataset.<br>\n",
    "One possible reason could be that taking random minibatches introduces the network to a large number of sets of images, thus leading to more network parameter updates.<br>\n",
    "This is also contributed by the fact that this small minibatch manages to capture the distribution of the entire dataset, thus leading to minimal information loss that helps to update the parameters successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we have learnt about shallow networks, and implemented a simple shallow feedforward network to classify MNIST handwritten images. We trained the whole network over multiple iterations, as well as trained it using minibatches and compared their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment completed by\n",
    " - Name: Anirudh Swaminathan\n",
    " - PID: A53316083\n",
    " - Email ID: aswamina@eng.ucsd.edu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
