{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook created by Anirudh Swaminathan from ECE department majoring in Intelligent Systems, Robotics and Control for the course ECE285 Machine Learning for Image Processing for Fall 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import MNISTtools\n",
    "help(MNISTtools.load)\n",
    "help(MNISTtools.show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "xtrain, ltrain = MNISTtools.load(path='./datasets/MNIST')\n",
    "print(xtrain.shape)\n",
    "print(ltrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of $xtrain$ is $(784, 60000)$<br>\n",
    "The shape of $ltrain$ is $(60000, )$<br>\n",
    "The size of the training set, i.e., the number of images in the training set is $60000$<br>\n",
    "The feature dimension is $784$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Displaying the image at index 42\n",
    "MNISTtools.show(xtrain[:, 42])\n",
    "\n",
    "# Print its corresponding label\n",
    "print(ltrain[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image at the index $42$ has been displayed.<br>\n",
    "The corresponding label has been printed and is found to be $7$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the range and type of xtrain\n",
    "min_x = np.amin(xtrain)\n",
    "max_x = np.amax(xtrain)\n",
    "\n",
    "print(\"Range of xtrain is from \", min_x, \" to \", max_x)\n",
    "print(\"Data type of xtrain is \", xtrain.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of values for $xtrain$ is from $0$ to $255$<br>\n",
    "The type of $xtrain$ is $uint8$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_MNIST_images(x):\n",
    "    # Convert the uint8 input into float32 for ease of normalization\n",
    "    fl_x = x.astype(np.float32)\n",
    "    \n",
    "    # Normalize [0 to 255] to [-1 to 1]\n",
    "    # This means mapping 0 to -1, 255 to 1, and 127.5 to 0\n",
    "    ret = 2*(fl_x - 255/2.0) / 255\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_x_train = normalize_MNIST_images(xtrain)\n",
    "print(norm_x_train.shape)\n",
    "print(\"Range of normalized xtrain is\", np.amin(norm_x_train), \"to\", np.amax(norm_x_train))\n",
    "print(\"Data type of normalized xtrain is\", norm_x_train.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrote the function to normalize the training data from $[0 to 255]$ to $[-1 to 1]$<br>\n",
    "We converted $xtrain$ which was of type $uint8$ into a vector of type $float32$<br>\n",
    "We then mapped $0$ to $-1$, $255$ to $1$ by subtracting the mid, which is $127.5$ and then dividing by mid, which is $127.5$<br>\n",
    "We then stored the normalized $xtrain$ in the variable $norm\\_x\\_train$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the code below\n",
    "def label2onehot(lbl):\n",
    "    # Creates a placeholder of size (10, 60000)\n",
    "    d = np.zeros((lbl.max() + 1, lbl.size))\n",
    "    \n",
    "    # One-hot encode the labels\n",
    "    d[lbl, np.arange(lbl.size)] = 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = label2onehot(ltrain)\n",
    "print(dtrain.shape)\n",
    "print(np.amin(dtrain), np.amax(dtrain))\n",
    "print(\"Label at index 42 is\", ltrain[42])\n",
    "print(\"Corresponding one-hot encodiing is\", dtrain[:, 42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one hot encoding line works as the $1^{st}$ index is traveresed independently of the $2^{nd}$ index<br>\n",
    "So, for each image given by the $2^{nd}$ axis, only the row index given by the value of the label is assigned $1$<br>\n",
    "Thus, $0$ maps to $[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]$ and 9 maps to $[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]$<br><br>\n",
    "We also checked the label for image $42$. The label is $7$ and the corresponding one-hot encoding is $[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot2label(d):\n",
    "    lbl = d.argmax(axis=0)\n",
    "    return lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if this works\n",
    "lab = dtrain[:, 42]\n",
    "che = onehot2label(lab)\n",
    "\n",
    "print(\"One-hot answer\", che, \"| Original:\", ltrain[42])\n",
    "assert(che == ltrain[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have thus checked if our implementation of recovering the label from one-hot encoding is correct<br>\n",
    "The label of the image at index at $42$ is $7$<br>\n",
    "The $onehot2label()$ function recovers this correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the softmax function\n",
    "def softmax(a):\n",
    "    # Calculate the max value\n",
    "    M = np.max(a, axis=0)\n",
    "    \n",
    "    # Subtract for easier exponential calculation\n",
    "    a_m = a - M\n",
    "    \n",
    "    # Calculate the exponent for each class for each image\n",
    "    exp_a_m = np.exp(a_m)\n",
    "    \n",
    "    # Calculate the sum for each class\n",
    "    den = np.sum(exp_a_m, axis=0)\n",
    "    \n",
    "    # Get the probabilities for each class for each image\n",
    "    g_a = exp_a_m / den\n",
    "    return g_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to show that $$\\frac{\\partial{g(a)_i}}{\\partial{a_i}} = g(a)_i(1 - g(a)_i)$$<br>\n",
    "By definition above, Softmax is $$y_i = g(a)_i = \\frac{exp(a_i)}{\\sum_{j=1}^{10}exp(a_j)} $$\n",
    "So, $$ \\frac{\\partial{g(a)_i}}{\\partial{a_i}} = \\frac{\\partial \\left({\\frac{exp(a_i)}{\\sum_{j=1}^{10}exp(a_j)}} \\right)}{\\partial{a_i}} $$\n",
    "Using the division rule of derivatives, we have $$ \\frac{\\partial{g(a)_i}}{\\partial{a_i}} = \\frac{\\sum_{j=1}^{10}exp(a_j)\\frac{\\partial{exp(a_i)}}{\\partial{a_i}} - exp(a_i)\\frac{\\partial \\left( {\\sum_{j=1}^{10}exp(a_j)} \\right)}{\\partial{a_i}}}{\\left( \\sum_{j=1}^{10}exp(a_j) \\right)^2} $$\n",
    "Simplifying, we have $$ \\frac{\\partial{g(a)_i}}{\\partial{a_i}} = \\frac{exp(a_i)* \\sum_{j=1}^{10}exp(a_j) - exp(a_i)*exp(a_i)}{\\left( \\sum_{j=1}^{10}exp(a_j) \\right)^2} $$\n",
    "Taking $\\frac{exp(a_i)}{\\sum_{j=1}^{10}exp(a_j)}$ outside, we have $$ \\frac{\\partial{g(a)_i}}{\\partial{a_i}} = \\frac{exp(a_i)}{\\sum_{j=1}^{10}exp(a_j)} * \\left( \\frac{\\sum_{j=1}^{10}exp(a_j) - exp(a_i)}{\\left( \\sum_{j=1}^{10}exp(a_j) \\right)} \\right) $$\n",
    "We know that $ g(a)_i = \\frac{exp(a_i)}{\\sum_{j=1}^{10}exp(a_j)} $<br>\n",
    "Thus, we have $$ \\frac{\\partial{g(a)_i}}{\\partial{a_i}} = g(a)_i * \\left( 1 - g(a)_i \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to show that $$\\frac{\\partial{g(a)_i}}{\\partial{a_j}} = -g(a)_i*g(a)_j for j\\neq i$$<br>\n",
    "By definition above, Softmax is $$y_i = g(a)_i = \\frac{exp(a_i)}{\\sum_{j=1}^{10}exp(a_j)} $$\n",
    "So, $$ \\frac{\\partial{g(a)_i}}{\\partial{a_j}} = \\frac{\\partial \\left({\\frac{exp(a_i)}{\\sum_{j=1}^{10}exp(a_j)}} \\right)}{\\partial{a_j}} $$\n",
    "Taking the term $exp(a_i)$ outside, we have, $$ \\frac{\\partial{g(a)_i}}{\\partial{a_j}} = exp(a_i) * \\frac{\\partial \\left({\\frac{1}{\\sum_{j=1}^{10}exp(a_j)}} \\right)}{\\partial{a_j}} $$\n",
    "Using inverse rule of derivatives, we have $$ \\frac{\\partial{g(a)_i}}{\\partial{a_j}} = exp(a_i) * \\frac{-1*exp(a_j)}{\\left( \\sum_{j=1}^{10}exp(a_j) \\right)^2} $$\n",
    "We know that $ g(a)_i = \\frac{exp(a_i)}{\\sum_{j=1}^{10}exp(a_j)} $<br>\n",
    "Thus, we have $$ \\frac{\\partial{g(a)_i}}{\\partial{a_j}} = -1*g(a)_i*g(a)_j for j\\neq i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - Jacobian is symmetric Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - Jacobian expression proof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the gradient of the softmax function\n",
    "# The directional derivative of the softmax function is as follows:-\n",
    "# delta = elementwise product (g(a) and e) - <g(a),e> g(a)\n",
    "def softmaxp(a, e):\n",
    "    # Calculate g(a)\n",
    "    g_a = softmax(a)\n",
    "    \n",
    "    # Calculate term 1\n",
    "    t1 = g_a * e\n",
    "    \n",
    "    # Calculate the directional derivative\n",
    "    delta = t1 - np.sum(t1, axis=0)*g_a\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if softmaxp is correct\n",
    "# finite difference step\n",
    "eps = 1e-6\n",
    "\n",
    "# random inputs\n",
    "a = np.random.randn(10, 200)\n",
    "\n",
    "# random directions\n",
    "e = np.random.randn(10, 200)\n",
    "\n",
    "# testing part\n",
    "diff = softmaxp(a, e)\n",
    "\n",
    "# From the definition of a derivative, we have\n",
    "diff_approx = (softmax(a + eps*e) - softmax(a)) / eps\n",
    "\n",
    "# Calculate the relative error of these 2 approaches\n",
    "rel_error = np.abs(diff - diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "\n",
    "# print the relative error\n",
    "print(rel_error, 'should be smaller than 1e-6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented the code to compute the directional derivative of $g$ at point $a$ in the direction of $e$ using the $softmaxp(a, e)$ function<br>\n",
    "We tested the implementation of our code by comparing with the fundamental definition of directional derivative, where, $$ \\delta = \\frac{\\partial g(a)}{\\partial a} \\times e = \\lim_{\\varepsilon\\to0} \\frac{g(a + \\varepsilon e) - g(a)}{\\varepsilon} $$\n",
    "We verified that our implementation of $softmaxp()$ is correct and that the relative error is smaller that $1e-6$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the ReLU(a) = max(ai, 0)\n",
    "def relu(a):\n",
    "    # Create a copy of the array a\n",
    "    g_a = np.copy(a)\n",
    "    \n",
    "    # Set those values less than 0 to 0\n",
    "    g_a[a < 0] = 0\n",
    "    return g_a\n",
    "\n",
    "def relup(a, e):\n",
    "    # Relup is the directional derivative of ReLU(a) in the direction of e\n",
    "    # Taking the Jacobian for ReLU and then deriving, we have found that the derivative is as given:-\n",
    "    # It is the element-wise product of gradient of relu and the vector e\n",
    "    # Create a copy of the array a\n",
    "    del_a = np.copy(a)\n",
    "    \n",
    "    # Set the values less than 0 to 0\n",
    "    del_a[a < 0] = 0\n",
    "    \n",
    "    # Set the values greater than 0 to 1\n",
    "    del_a[a > 0] = 1\n",
    "    \n",
    "    # Compute delta as the element-wise product of the gradient of relu and the vector e\n",
    "    delta = del_a * e\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented the relu function and its directional derivative now<br>\n",
    "We used the Jacobian to derive the relation of relup to vector operations<br>\n",
    "We shall now test $reulp()$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if relup is correct\n",
    "# finite difference step\n",
    "eps = 1e-6\n",
    "\n",
    "# random inputs\n",
    "a = np.random.randn(10, 200)\n",
    "\n",
    "# random directions\n",
    "e = np.random.randn(10, 200)\n",
    "\n",
    "# testing part\n",
    "diff = relup(a, e)\n",
    "\n",
    "# From the definition of a derivative, we have\n",
    "diff_approx = (relu(a + eps*e) - relu(a)) / eps\n",
    "\n",
    "# Calculate the relative error of these 2 approaches\n",
    "rel_error = np.abs(diff - diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "\n",
    "# print the relative error\n",
    "print(rel_error, 'should be smaller than 1e-6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented the code to compute the directional derivative of $g$ at point $a$ in the direction of $e$ using the $relup(a, e)$ function<br>\n",
    "We tested the implementation of our code by comparing with the fundamental definition of directional derivative, where, $$ \\delta = \\frac{\\partial g(a)}{\\partial a} \\times e = \\lim_{\\varepsilon\\to0} \\frac{g(a + \\varepsilon e) - g(a)}{\\varepsilon} $$\n",
    "We verified that our implementation of $relup()$ is correct and that the relative error is smaller that $1e-6$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
