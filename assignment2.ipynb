{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - CNNs and PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name: Anirudh Swaminathan\n",
    "### PID: A53316083\n",
    "### Email ID: aswamina@ucsd.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook created by Anirudh Swaminathan from ECE department majoring in Intelligent Systems, Robotics and Control for the course ECE285 Machine Learning for Image Processing for Fall 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct 5*3 tensor and print it\n",
    "x = torch.Tensor(5, 3)\n",
    "print(x)\n",
    "\n",
    "# printing its type\n",
    "print(type(x))\n",
    "\n",
    "# printing its data type\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x$ was randomly initialized. $x$ is of type $\\textbf{torch.Tensor}$ and it's data is of type $\\text{torch.float32}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(y)\n",
    "\n",
    "# Finding the type of y\n",
    "print(type(y))\n",
    "print(y.dtype)\n",
    "\n",
    "# Using randn() instead of rand()\n",
    "y1 = torch.randn(5, 3)\n",
    "print(y1)\n",
    "print(type(y1))\n",
    "print(y1.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y$ is a $(5, 3)$ tensor with random values distributed in a uniform distribution from $0$ to $1$<br>\n",
    "$y$ is of type $\\textbf{torch.Tensor}$ and it's data is of type torch.float32<br>\n",
    "$y_1$ is a $(5, 3)$ tensor with random values distributed as a Gaussian with mean $0$ and variance $1$. So, $y_1$ is a tensor filled with random values from a standard normal distribution<br>\n",
    "So, if we use $torch.randn()$ function instead of $torch.rand()$, we may get negative values for $torch.randn()$ but not for $torch.rand()$ function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.double()\n",
    "y = y.double()\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type displayed when we print $x$ and $y$ are $torch.float64$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tensors with values directly\n",
    "x = torch.Tensor([[-0.1859, 1.3970, 0.5236],\n",
    "[ 2.3854, 0.0707, 2.1970],\n",
    "[-0.3587, 1.2359, 1.8951],\n",
    "[-0.1189, -0.1376, 0.4647],\n",
    "[-1.8968, 2.0164, 0.1092]])\n",
    "\n",
    "y = torch.Tensor([[ 0.4838, 0.5822, 0.2755],\n",
    "[ 1.0982, 0.4932, -0.6680],\n",
    "[ 0.7915, 0.6580, -0.5819],\n",
    "[ 0.3825, -1.1822, 1.5217],\n",
    "[ 0.6042, -0.2280, 1.3210]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the shapes of the two tensors x and y\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of $x$ is $(5, 3)$. <br>\n",
    "Shape of $y$ is $(5, 3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack 2 tensors\n",
    "z = torch.stack((x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z, z.dtype, z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, compare it with torch.cat()\n",
    "z1 = torch.cat((x, y), 0)\n",
    "z2 = torch.cat((x, y), 1)\n",
    "print(z1, z1.shape)\n",
    "print(z2, z2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the tensor $z$ is $(2, 5, 3)$. <br>\n",
    "$torch.stack()$ stacks its arguments on a new dimension, i.e., on top of one another in this case. <br>\n",
    "\n",
    "We also compared it to $torch.cat()$. <br>\n",
    "In $torch.cat((x, y), 0)$, the tensor $y$ is concatenated to tensor $x$ along axis $0$, i.e., it is concatenated along the row. This results in a tensor that is of the shape $(10, 3)$ that is obtained by combining the two tensors, each of shape $(5, 3)$ row-wise. The output of this $torch.cat()$ is still the same dimension $(2D)$. <br>\n",
    "\n",
    "Similarly, in $torch.cat((x, y), 1)$, the tensor $y$ is concatenated to tensor $x$ along axis $1$, i.e., it is concatenated along the column. This results in a tensor that is of the shape $(5, 6)$ that is obtained by combining the two tensors, each of shape $(5, 3)$ column-wise. The output of this $torch.cat()$ is still the same dimension $(2D)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the element of the 5th row and 3rd column in 2d tensor y\n",
    "ele = y[4, 2]\n",
    "print(\"The element of the 5th row and 3rd column in 2d tensor y:\", ele.item())\n",
    "\n",
    "# Accessing the same element in the 3D tensor z\n",
    "ele_3d = z[1, 4, 2]\n",
    "print(\"Accessing the same element in the 3d tensor z, we have\", ele_3d.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we were able to access the element of the $5^{th}$ row and $3^{rd}$ column in the $2D$ tensor $y$. <br>\n",
    "We were also able to access the same element from the $3D$ tensor $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all elements corresponding to the 5th row and 3rd column in z\n",
    "eles = z[:, 4, 2]\n",
    "print(\"Printing all elements corresponding to the 5th row and 3rd column in z:\", eles)\n",
    "print(eles.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are $2$ elements in $z$ that correspond to the $5^{th}$ row and $3^{rd}$ column of the tensor $z$. <br>\n",
    "This is beacause $z$ is the stacked tensor of $x$ and $y$. Hence, the $1^{st}$ returned element corresponds to the element at the $5^{th}$ row and $3^{rd}$ column of the tensor $x$, and the $2^{nd}$ returned element corresponds to the element at the $5^{th}$ row and $3^{rd}$ column of the tensor $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x + y)\n",
    "print(torch.add(x, y))\n",
    "print(x.add(y))\n",
    "torch.add(x, y, out=x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the $4$ methods of addition print the same output. <br>\n",
    "Also, all the $4$ methods are equivalent. They all take in 2 tensors x and y, and then output a new tensor. <br>\n",
    "They all do NOT modify the tensors $x$ and $y$. <br>\n",
    "Tensor $x$ seems modified in the last statement only because $out=x$ was specified, which meant that $torch$ stored the output of the addition operation between $x$ and $y$ in the variable $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensor whose values are sampled from a Normal Distribution with mean 0 and variance 1\n",
    "x = torch.randn(4, 4)\n",
    "\n",
    "# store a reshaped version of x in y such that it is a 1D tensor of size 16\n",
    "y = x.view(16)\n",
    "\n",
    "# store the reshaped version of x in z such that it is a 2D tensor of size (2, 8)\n",
    "z = x.view(-1, 8)\n",
    "print(x.size(), y.size(), z.size())\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $1^{st}$ statement creates tensor $x$ of shape $(4, 4)$ using the $randn()$ function whose values are sampled from a Normal Distribution with $\\mu = 0$ and $\\sigma^2 = 1$. <br>\n",
    "The $2^{nd}$ statement stores a reshaped version of $x$ in $y$ such that it is a $1D$ tensor of size $16$. <br>\n",
    "The $3^{rd}$ statement stores a reshaped version of $x$ in $z$ such that it is a $2D$ tensor of size $(2, 8)$. <br>\n",
    "The $-1$ in the argument for the $view()$ function states that the $1^{st}$ dimension of $z$ should be inferred by $torch$ directly given that the $2^{nd}$ dimension of $z$ is $8$. <br>\n",
    "This conversion is just $4 * 4 = 16; \\frac{16}{8} = 2$. Thus, the $1^{st}$ dimension of $z$ should be $2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random x of dimension 10*10\n",
    "x = torch.randn(10, 10)\n",
    "\n",
    "# Generate random y of dimension 2*100\n",
    "y = torch.randn(2, 100)\n",
    "print(x.size(), y.size())\n",
    "\n",
    "# reshape x to become a row vector and make it compatible for matrix multiplication\n",
    "x = x.view(1, 100)\n",
    "\n",
    "# reshape y to become a matrix compatible for matrix multiplication with x\n",
    "y = y.view(100, 2)\n",
    "print(x.size(), y.size())\n",
    "\n",
    "# perform row vector by matrix multiplication\n",
    "z = torch.mm(x, y)\n",
    "print(z.size())\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a tensor $x$ of size $(10, 10)$ and tensor $y$ of size $(2, 100)$. <br>\n",
    "We then reshaped the tensor $x$ to row vector of size $(1, 100)$. <br>\n",
    "Tensor $y$ was also reshaped to size $(100, 2)$ to make it conformable for matrix multiplication with $x$. <br>\n",
    "Finally, the result of the matrix multiplication carried out by $torch.mm(x, y)$ is stored in the tensor $z$. <br>\n",
    "Tensor $z$ is of size $(1, 100)*(100, 2) = (1, 2)$. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy and PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "print(b)\n",
    "\n",
    "print(type(a), type(b))\n",
    "print(a.dtype, b.dtype)\n",
    "print(a.size(), b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable $a$ is a $1D$ tensor of size $(5)$ carrying data of type $torch.float32$. Variable $b$ is a $1D$ numpy array of shape $(5,)$ carrying data of type $float32$. <br>\n",
    "$b$ is the $numpy$ version of tensor $a$ and both carry the same data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0] += 1\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor $a$ and numpy array $b$ both share the same underlying memory location. <br>\n",
    "Modifying $a$ changes $b$ and modifying $b$ changes $a$ if the tensor $a$ is on the CPU, which is the case here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $add\\_(1)$ modifies $a$ in-place, thus modifying numpy array $b$ also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:] += 1\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This statement modifies $a$, thus modifying numpy $b$ also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.add(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The satement $a.add(1)$ adds $1$ to $a$ and returns a new tensor. <br>\n",
    "Since we store the result in $a$, only $a$ is now the variable that points to the new tensor output, but the underlying memory location is not modified. Thus, $b$ is not modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy array $a$ and tensor $b$ share the same underlying location. <br>\n",
    "Modifying $a$ changes $b$ and vice-versa if the tensor is in the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU experiments\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# Create a tensor on CPU and then move it to GPU\n",
    "x = torch.randn(5, 3).to(device)\n",
    "\n",
    "# Create a tensor directly on the GPU\n",
    "y = torch.randn(5, 3, device=device)\n",
    "z = x + y\n",
    "print(x.size(), x.dtype, x.device)\n",
    "print(y.size(), y.dtype, y.device)\n",
    "print(z.size(), z.dtype, z.device)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor $x$ is first created in the CPU and then transferred to the GPU using the $.to()$ command. <br>\n",
    "Tensor $y$ is created in the GPU directly with the $device$ argument in the $randn()$ function. <br>\n",
    "I feel that the allocation instruction for $y$ is more efficient than the one for $x$ as creating a tensor on CPU and then transferring it to GPU is $2$ steps, with the additional overhead of transferring it between devices. <br>\n",
    "Directly allocating the tensor to the GPU would avoid these extra steps, and hence would be more efficient comparatively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line runs fine\n",
    "print(z.cpu().numpy())\n",
    "\n",
    "# The following line produces an error\n",
    "print(z.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the $1^{st}$ line, the tensor $z$ is copied to CPU first using the $.cpu()$ function. Then, it is converted to a numpy array in the CPU. <br>\n",
    "The $2^{nd}$ line throws a $TypeError$ as $torch$ can't convert the CUDA tensor to numpy directly. It means that the conversion has to be carried out in the CPU, and hence $z.cpu()$ has to be used first before the conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd: automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.requires_grad)\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(x.grad_fn)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the $requires\\_grad$ attribute of $x$ is $True$ and we are performing operations on $x$ to obtain $y$, $y$ will have its attribute $requires\\_grad$ set to $True$ automatically. <br>\n",
    "The $grad$ attributes of both the tensors $x$ and $y$ are $None$. This is because the gradient has not been computed yet for these tensors using the $.backward()$ function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y * y * 3\n",
    "f = z.mean()\n",
    "print(z, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
